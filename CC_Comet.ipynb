{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guiOsorio/Learning_JAX/blob/master/CC_Comet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Diw1aeurAJwt"
      },
      "source": [
        "- Focal loss: https://www.youtube.com/watch?v=Y8_OVwK4ECk\n",
        "- Focal loss for PyTorch: https://github.com/AdeelH/pytorch-multi-class-focal-loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GOALS\n",
        "- implement a neural network architecture in FLAX\n",
        "  - adam optimizer\n",
        "  - implement regularization techniques:\n",
        "    - L2 regularization\n",
        "    - dropout\n",
        "    - batch norm\n",
        "\n",
        "\n",
        "## DATA\n",
        "- consists of principal components obtained when performing PCA on a SMS credit card fraud dataset\n",
        "- labels are ham -> 0 or spam -> 1\n",
        "- since most SMS messages are not spams, this dataset is highly imbalanced\n",
        "\n",
        "\n",
        "## CHALLENGE\n",
        "- due to the nature of the dataset -> find techniques to improve recall of the model\n",
        "\n",
        "#### Supplementary notebook - https://github.com/guiOsorio/Learning_JAX/blob/master/CC_FocalLoss.ipynb"
      ],
      "metadata": {
        "id": "4TUk6PBlb-Z5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YZMP5OhU5zja",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6db21281-65f6-4cc5-c7c5-d7ecae658e1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -ax (/usr/local/lib/python3.8/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ax (/usr/local/lib/python3.8/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ax (/usr/local/lib/python3.8/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ax (/usr/local/lib/python3.8/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ax (/usr/local/lib/python3.8/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: jax 0.4.1 does not provide the extra 'cuda11_cudnn805'\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ax (/usr/local/lib/python3.8/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ax (/usr/local/lib/python3.8/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ax (/usr/local/lib/python3.8/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ax (/usr/local/lib/python3.8/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install Comet\n",
        "!pip install comet_ml --quiet\n",
        "# Install Flax and JAX\n",
        "!pip install --upgrade -q \"jax[cuda11_cudnn805]\" -f https://storage.googleapis.com/jax-releases/jax_releases.html\n",
        "!pip install --upgrade -q git+https://github.com/google/flax.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nfitRb8j55XE"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "from jax import lax, random, jit, numpy as jnp\n",
        "\n",
        "import flax\n",
        "from flax.core import freeze, unfreeze\n",
        "from flax import linen as nn\n",
        "from flax.training import train_state\n",
        "\n",
        "import optax\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import functools\n",
        "from typing import Sequence, Callable, Any, Optional\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "kRgD8Vy76hnG",
        "outputId": "305d4f07-c72a-413f-ad85-0f5732a92dcc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
              "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
              "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
              "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
              "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
              "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
              "\n",
              "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
              "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
              "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
              "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
              "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
              "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
              "\n",
              "        V26       V27       V28  Amount  Class  \n",
              "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
              "1  0.125895 -0.008983  0.014724    2.69      0  \n",
              "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
              "3 -0.221929  0.062723  0.061458  123.50      0  \n",
              "4  0.502292  0.219422  0.215153   69.99      0  \n",
              "\n",
              "[5 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dca70e57-66d0-4744-9835-96ec4ebe0e99\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>...</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>149.62</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>...</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>378.66</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.966272</td>\n",
              "      <td>-0.185226</td>\n",
              "      <td>1.792993</td>\n",
              "      <td>-0.863291</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>1.247203</td>\n",
              "      <td>0.237609</td>\n",
              "      <td>0.377436</td>\n",
              "      <td>-1.387024</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.108300</td>\n",
              "      <td>0.005274</td>\n",
              "      <td>-0.190321</td>\n",
              "      <td>-1.175575</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>-0.221929</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>123.50</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.0</td>\n",
              "      <td>-1.158233</td>\n",
              "      <td>0.877737</td>\n",
              "      <td>1.548718</td>\n",
              "      <td>0.403034</td>\n",
              "      <td>-0.407193</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.592941</td>\n",
              "      <td>-0.270533</td>\n",
              "      <td>0.817739</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.009431</td>\n",
              "      <td>0.798278</td>\n",
              "      <td>-0.137458</td>\n",
              "      <td>0.141267</td>\n",
              "      <td>-0.206010</td>\n",
              "      <td>0.502292</td>\n",
              "      <td>0.219422</td>\n",
              "      <td>0.215153</td>\n",
              "      <td>69.99</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 31 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dca70e57-66d0-4744-9835-96ec4ebe0e99')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dca70e57-66d0-4744-9835-96ec4ebe0e99 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dca70e57-66d0-4744-9835-96ec4ebe0e99');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Load data\n",
        "url = 'https://raw.githubusercontent.com/nsethi31/Kaggle-Data-Credit-Card-Fraud-Detection/master/creditcard.csv'\n",
        "df = pd.read_csv(url)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAaz5NwP6jAZ",
        "outputId": "06f1bae3-f579-468f-f57e-42a7e2b11598"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([227846, 3]),\n",
              " torch.Size([227846]),\n",
              " torch.Size([56961, 3]),\n",
              " torch.Size([56961]))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "class CustomTensorDataset(Dataset):\n",
        "  def __init__(self, dataset):\n",
        "    [data_X, data_y] = dataset\n",
        "    X_tensor, y_tensor = data_X, data_y\n",
        "    tensors = (X_tensor, y_tensor)\n",
        "    assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n",
        "    self.tensors = tensors\n",
        "    self.data = tensors[0]\n",
        "    self.targets = tensors[1]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    x = self.tensors[0][index]\n",
        "\n",
        "    y = self.tensors[1][index]\n",
        "\n",
        "    return x, y\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.tensors[0].size(0)\n",
        "\n",
        "# Divide into features and labels\n",
        "df_x = df.iloc[:, 1:4]\n",
        "df_y = df['Class'].to_frame()\n",
        "\n",
        "total_points = df_y.shape[0]\n",
        "split = round(total_points*0.8)\n",
        "\n",
        "# Convert pd.dataframes to tensors\n",
        "train_x = torch.tensor(df_x.values, dtype=torch.float32)[:split]\n",
        "train_y = torch.squeeze(torch.tensor(df_y.values, dtype=torch.float32)[:split])\n",
        "\n",
        "test_x = torch.tensor(df_x.values, dtype=torch.float32)[split:]\n",
        "test_y = torch.squeeze(torch.tensor(df_y.values, dtype=torch.float32)[split:])\n",
        "\n",
        "train_x.size(), train_y.size(), test_x.size(), test_y.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1ms7JWgI57gi"
      },
      "outputs": [],
      "source": [
        "# Transform tensors to np arrays in dataloaders, tensors not compatible with JAX\n",
        "def custom_collate_fn(batch):\n",
        "    transposed_data = list(zip(*batch))\n",
        "\n",
        "    labels = np.stack(transposed_data[1])\n",
        "    features = np.stack(transposed_data[0])\n",
        "\n",
        "    return features, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ezBYGH8v6Kl-"
      },
      "outputs": [],
      "source": [
        "# Implementation with batch norm and dropout\n",
        "class NN_regularized(nn.Module):\n",
        "\n",
        "  @nn.compact \n",
        "  def __call__(self, x, train: bool):\n",
        "    # Linear + dropout + relu\n",
        "    x = nn.Dense(features=100)(x)\n",
        "    x = nn.Dropout(0.2, deterministic=not train)(x)\n",
        "    x = nn.relu(x)\n",
        "\n",
        "    # Linear + batch norm + relu\n",
        "    x = nn.Dense(features=256)(x)\n",
        "    x = nn.BatchNorm(use_running_average=not train)(x)\n",
        "    x = nn.relu(x)\n",
        "\n",
        "    # Linear + softmax\n",
        "    x = nn.Dense(features=2)(x)\n",
        "    x = nn.softmax(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZK4RQnMpJ4Xy",
        "outputId": "7ae86879-da99-4846-cb7f-e272ed6fa964"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(128, 3)\n",
            "(128,)\n",
            "(56961, 3)\n",
            "(56961,)\n"
          ]
        }
      ],
      "source": [
        "input_size = (1, 3)\n",
        "batch_size = 128\n",
        "# Replicate each TP in the training set n times\n",
        "def mod_data(n): # higher n => higher recall, lower precision?\n",
        "  TP_idxs = (train_y == 1).nonzero(as_tuple=True)[0]\n",
        "\n",
        "  extra_xs = train_x[TP_idxs].repeat(n, 1)\n",
        "  extra_ys = train_y[TP_idxs].repeat(n,)\n",
        "\n",
        "  train_x_mod = torch.cat((train_x, extra_xs), 0)\n",
        "  train_y_mod = torch.cat((train_y, extra_ys), 0)\n",
        "\n",
        "  train_mod = [train_x_mod, train_y_mod]\n",
        "  train_dset_mod = CustomTensorDataset(train_mod)\n",
        "  train_loader_mod = DataLoader(train_dset_mod, collate_fn=custom_collate_fn, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  test = [test_x, test_y]\n",
        "  test_dset = CustomTensorDataset(test)\n",
        "  test_loader = DataLoader(test_dset, collate_fn=custom_collate_fn, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  # optimization - loading the whole dataset into memory\n",
        "  train_features = jnp.array(train_dset_mod.data)\n",
        "  train_lbls = jnp.array(train_dset_mod.targets)\n",
        "\n",
        "  # np.expand_dims is to convert shape from (10000, 28, 28) -> (10000, 28, 28, 1)\n",
        "  # We don't have to do this for training images because custom_transform does it for us.\n",
        "  test_features = jnp.array(test_dset.data)\n",
        "  test_lbls = jnp.array(test_dset.targets)\n",
        "\n",
        "  return train_loader_mod, test_features, test_lbls\n",
        "\n",
        "## Create test loader\n",
        "tlm_test, tf_test, tl_test = mod_data(10)\n",
        "for data in tlm_test:\n",
        "  x, y = data\n",
        "  print(x.shape)\n",
        "  print(y.shape)\n",
        "  break\n",
        "\n",
        "print(tf_test.shape)\n",
        "print(tl_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vRFiWVPyJ7qT"
      },
      "outputs": [],
      "source": [
        "# TRAINING\n",
        "\n",
        "# Compute loss and update - this will be computed many times, so it's best to jit it\n",
        "@jit\n",
        "def training_state(state, imgs, gt_labels):\n",
        "\n",
        "  def FocalLoss(params, batch_stats):\n",
        "    probs, updates = NN_regularized().apply({'params': params, 'batch_stats': batch_stats}, imgs, train=True, rngs={'dropout': jax.random.PRNGKey(0)}, mutable=['batch_stats'])\n",
        "    logits = jnp.log10(probs)\n",
        "    # logits is a vector of probabilities predicted by the model (the highest value in the vector is the prediction)\n",
        "    one_hot_gt_labels = jax.nn.one_hot(gt_labels, num_classes=2) # one hot encoded vector of labels \n",
        "    # logits.shape and one_hot_gt_labels shape is (batch_size, num_classes)\n",
        "    # Cross entropy to focal loss -> -log(pt) TO -log(pt) * (1-pt)^(gamma)\n",
        "    # above is -log(pt), need to find (1-pt)^gamma. log10(x) = 2 ==> x = 10^2, therefore log10(x) = logit ==> x = 10^logit\n",
        "    gamma = 2\n",
        "    focal_loss = -jnp.mean(jnp.sum((logits * jnp.power(1-probs, gamma)) * one_hot_gt_labels, axis=-1)) \n",
        "    # axis=-1 means sum over rows ||-> CE = true probability (one hot gt labels) * predicted probability (logits)\n",
        "    \n",
        "\n",
        "    # Add l2 regularization\n",
        "    alpha = 0.1\n",
        "    def l2_loss(weights, alpha):\n",
        "      return alpha * (weights ** 2).mean()\n",
        "    \n",
        "    focal_loss += sum(\n",
        "        l2_loss(w, alpha)\n",
        "        for w in jax.tree_util.tree_leaves(params)\n",
        "      )\n",
        "\n",
        "    return focal_loss, (logits, updates)\n",
        "  \n",
        "  (loss, (logits, updates)), grads = jax.value_and_grad(FocalLoss, argnums=0, has_aux=True)(state.params, state.batch_stats)\n",
        "  state = state.apply_gradients(grads=grads) # update state params based on grads calculated\n",
        "  state = state.replace(batch_stats=updates['batch_stats']) # update state batch_stats variables\n",
        "\n",
        "  ## Accuracy\n",
        "  accuracy = jnp.mean(jnp.argmax(logits, -1) == gt_labels)\n",
        "  metrics = {\n",
        "      'loss': loss,\n",
        "      'accuracy': accuracy\n",
        "  }\n",
        "\n",
        "  return state, metrics\n",
        "\n",
        "# One epoch - need to add metrics part\n",
        "def train_one_epoch(state, dataloader):\n",
        "  batch_metrics = []\n",
        "  for cnt, (imgs, labels) in enumerate(dataloader):\n",
        "    state, metrics = training_state(state, imgs, labels)\n",
        "    batch_metrics.append(metrics)\n",
        "\n",
        "  batch_metrics_np = jax.device_get(batch_metrics)  # pull from the accelerator onto host (CPU)\n",
        "  epoch_metrics_np = {\n",
        "      k: np.mean([metrics[k] for metrics in batch_metrics_np])\n",
        "      for k in batch_metrics_np[0]\n",
        "  }\n",
        "\n",
        "  return state, epoch_metrics_np\n",
        "\n",
        "def create_train_state(key, lr):\n",
        "  # Create model\n",
        "  NN = NN_regularized()\n",
        "  # Initialize parameters\n",
        "  variables = NN.init(key, jnp.ones([1, *input_size]), train=False)\n",
        "  params = variables['params']\n",
        "  batch_stats_v = variables['batch_stats']\n",
        "  del variables\n",
        "\n",
        "  class TrainState_stats(train_state.TrainState):\n",
        "    batch_stats: Any\n",
        "\n",
        "  state = TrainState_stats.create(\n",
        "    apply_fn=NN.apply,\n",
        "    params=params,\n",
        "    batch_stats=batch_stats_v,\n",
        "    tx=optax.adam(lr)\n",
        "  )\n",
        "\n",
        "  return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HtFS43HIQzjQ"
      },
      "outputs": [],
      "source": [
        "# EVALUATION\n",
        "\n",
        "# Run one evaluation on test set\n",
        "@jit\n",
        "def eval_step(state, imgs, gt_labels):\n",
        "  probs = NN_regularized().apply({'params': state.params, 'batch_stats': state.batch_stats}, imgs, rngs={'dropout': jax.random.PRNGKey(0)}, train=False)\n",
        "  logits = jnp.log10(probs)\n",
        "  one_hot_gt_labels = jax.nn.one_hot(gt_labels, num_classes=2)\n",
        "  gamma = 2\n",
        "  focal_loss = -jnp.mean(jnp.sum((logits * jnp.power(1-probs, gamma)) * one_hot_gt_labels, axis=-1)) \n",
        "  preds = jnp.argmax(logits, -1)\n",
        "  accuracy = jnp.mean(preds == gt_labels)\n",
        "  metrics = {\n",
        "      'loss': focal_loss,\n",
        "      'accuracy': accuracy\n",
        "  }\n",
        "  return metrics, preds, logits\n",
        "\n",
        "def evaluate_model(state, test_imgs, test_labels):\n",
        "  metrics, preds, logits = eval_step(state, test_imgs, test_labels)\n",
        "  metrics = jax.device_get(metrics) # pull from accelerator to CPU\n",
        "  metrics = jax.tree_map(lambda x: x.item(), metrics) # get scalar value from array\n",
        "  return metrics, preds, logits"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replicate each TP in the training set n times\n",
        "n = 1 # higher n => higher recall, lower precision -- AS SEEN IN COMET ANALYSIS\n",
        "\n",
        "TP_idxs = (train_y == 1).nonzero(as_tuple=True)[0]\n",
        "print(f'Percentage of TPs in original training data -> {(TP_idxs.shape[0] / train_y.shape[0]) * 100} %')\n",
        "\n",
        "xs_toadd = train_x[TP_idxs].repeat(n, 1)\n",
        "ys_toadd = train_y[TP_idxs].repeat(n,)\n",
        "\n",
        "train_x_add = torch.cat((train_x, xs_toadd), 0)\n",
        "train_y_add = torch.cat((train_y, ys_toadd), 0)\n",
        "\n",
        "print(f'Old shapes: {train_x.shape}, {train_y.shape}')\n",
        "print(f'New shapes: {train_x_add.shape}, {train_y_add.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBb5y0fMX2g_",
        "outputId": "e25d9f20-2410-4f25-b27b-98341ac4b9e9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage of TPs in original training data -> 0.18301835450260262 %\n",
            "Old shapes: torch.Size([227846, 3]), torch.Size([227846])\n",
            "New shapes: torch.Size([228263, 3]), torch.Size([228263])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpzyATHNQ3Tq",
        "outputId": "bf59d46a-3479-400a-9ac2-07a2e1a4492a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: sklearn, torch. Metrics and hyperparameters can still be logged using Experiment.log_metrics() and Experiment.log_parameters()\n",
            "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
            "COMET ERROR: Failed to calculate active processors count. Fall back to default CPU count 1\n",
            "COMET INFO: Couldn't find a Git repository in '/content' nor in any parent directory. You can override where Comet is looking for a Git Patch by setting the configuration `COMET_GIT_DIRECTORY`\n",
            "COMET INFO: Experiment is live on comet.com https://www.comet.com/guiosorio/credit-card-fraud-curves/90742ee731274909b3c7200df3802da1\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "COMET INFO: ---------------------------\n",
            "COMET INFO: Comet.ml Experiment Summary\n",
            "COMET INFO: ---------------------------\n",
            "COMET INFO:   Data:\n",
            "COMET INFO:     display_summary_level : 1\n",
            "COMET INFO:     url                   : https://www.comet.com/guiosorio/credit-card-fraud-curves/90742ee731274909b3c7200df3802da1\n",
            "COMET INFO:   Metrics [count] (min, max):\n",
            "COMET INFO:     accuracy [4]  : (0.9986656904220581, 0.9987183809280396)\n",
            "COMET INFO:     loss [4]      : (0.0010010512778535485, 0.0015405886806547642)\n",
            "COMET INFO:     precision [4] : (0.0, 0.6666666666666666)\n",
            "COMET INFO:     recall [4]    : (0.0, 0.05333333333333334)\n",
            "COMET INFO:   Others:\n",
            "COMET INFO:     Name : 1 Repeats\n",
            "COMET INFO:   Parameters:\n",
            "COMET INFO:     Repeats : 1\n",
            "COMET INFO:   Uploads:\n",
            "COMET INFO:     confusion-matrix    : 1\n",
            "COMET INFO:     environment details : 1\n",
            "COMET INFO:     filename            : 1\n",
            "COMET INFO:     installed packages  : 1\n",
            "COMET INFO:     notebook            : 2\n",
            "COMET INFO:     os packages         : 1\n",
            "COMET INFO:     source_code         : 1\n",
            "COMET INFO: ---------------------------\n",
            "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: sklearn, torch. Metrics and hyperparameters can still be logged using Experiment.log_metrics() and Experiment.log_parameters()\n",
            "COMET INFO: Uploading metrics, params, and assets to Comet before program termination (may take several seconds)\n",
            "COMET INFO: The Python SDK has 3600 seconds to finish before aborting...\n",
            "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: sklearn, torch. Metrics and hyperparameters can still be logged using Experiment.log_metrics() and Experiment.log_parameters()\n",
            "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
            "COMET ERROR: Failed to calculate active processors count. Fall back to default CPU count 1\n",
            "COMET INFO: Couldn't find a Git repository in '/content' nor in any parent directory. You can override where Comet is looking for a Git Patch by setting the configuration `COMET_GIT_DIRECTORY`\n",
            "COMET INFO: Experiment is live on comet.com https://www.comet.com/guiosorio/credit-card-fraud-curves/c0720b92630b4dd3b8e127efece88f0e\n",
            "\n",
            "COMET INFO: ---------------------------\n",
            "COMET INFO: Comet.ml Experiment Summary\n",
            "COMET INFO: ---------------------------\n",
            "COMET INFO:   Data:\n",
            "COMET INFO:     display_summary_level : 1\n",
            "COMET INFO:     url                   : https://www.comet.com/guiosorio/credit-card-fraud-curves/c0720b92630b4dd3b8e127efece88f0e\n",
            "COMET INFO:   Metrics [count] (min, max):\n",
            "COMET INFO:     accuracy [4]  : (0.996313214302063, 0.9987183809280396)\n",
            "COMET INFO:     loss [4]      : (0.001615113578736782, 0.0039546373300254345)\n",
            "COMET INFO:     precision [4] : (0.18045112781954886, 0.5238095238095238)\n",
            "COMET INFO:     recall [4]    : (0.29333333333333333, 0.5333333333333333)\n",
            "COMET INFO:   Others:\n",
            "COMET INFO:     Name : 6 Repeats\n",
            "COMET INFO:   Parameters:\n",
            "COMET INFO:     Repeats : 6\n",
            "COMET INFO:   Uploads:\n",
            "COMET INFO:     confusion-matrix    : 1\n",
            "COMET INFO:     environment details : 1\n",
            "COMET INFO:     filename            : 1\n",
            "COMET INFO:     installed packages  : 1\n",
            "COMET INFO:     notebook            : 2\n",
            "COMET INFO:     os packages         : 1\n",
            "COMET INFO:     source_code         : 1\n",
            "COMET INFO: ---------------------------\n",
            "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: sklearn, torch. Metrics and hyperparameters can still be logged using Experiment.log_metrics() and Experiment.log_parameters()\n",
            "COMET INFO: Uploading 1 metrics, params and output messages\n",
            "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: sklearn, torch. Metrics and hyperparameters can still be logged using Experiment.log_metrics() and Experiment.log_parameters()\n",
            "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
            "COMET ERROR: Failed to calculate active processors count. Fall back to default CPU count 1\n",
            "COMET INFO: Couldn't find a Git repository in '/content' nor in any parent directory. You can override where Comet is looking for a Git Patch by setting the configuration `COMET_GIT_DIRECTORY`\n",
            "COMET INFO: Experiment is live on comet.com https://www.comet.com/guiosorio/credit-card-fraud-curves/31353250c8c1422a9d4a25e6e0e84884\n",
            "\n",
            "COMET INFO: ---------------------------\n",
            "COMET INFO: Comet.ml Experiment Summary\n",
            "COMET INFO: ---------------------------\n",
            "COMET INFO:   Data:\n",
            "COMET INFO:     display_summary_level : 1\n",
            "COMET INFO:     url                   : https://www.comet.com/guiosorio/credit-card-fraud-curves/31353250c8c1422a9d4a25e6e0e84884\n",
            "COMET INFO:   Metrics [count] (min, max):\n",
            "COMET INFO:     accuracy [4]  : (0.9951193928718567, 0.9990168213844299)\n",
            "COMET INFO:     loss [4]      : (0.0018532619578763843, 0.0037658540531992912)\n",
            "COMET INFO:     precision [4] : (0.1387900355871886, 0.8421052631578947)\n",
            "COMET INFO:     recall [4]    : (0.21333333333333335, 0.52)\n",
            "COMET INFO:   Others:\n",
            "COMET INFO:     Name : 11 Repeats\n",
            "COMET INFO:   Parameters:\n",
            "COMET INFO:     Repeats : 11\n",
            "COMET INFO:   Uploads:\n",
            "COMET INFO:     confusion-matrix    : 1\n",
            "COMET INFO:     environment details : 1\n",
            "COMET INFO:     filename            : 1\n",
            "COMET INFO:     installed packages  : 1\n",
            "COMET INFO:     notebook            : 2\n",
            "COMET INFO:     os packages         : 1\n",
            "COMET INFO:     source_code         : 1\n",
            "COMET INFO: ---------------------------\n",
            "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: sklearn, torch. Metrics and hyperparameters can still be logged using Experiment.log_metrics() and Experiment.log_parameters()\n",
            "COMET INFO: Uploading 1 metrics, params and output messages\n",
            "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: sklearn, torch. Metrics and hyperparameters can still be logged using Experiment.log_metrics() and Experiment.log_parameters()\n",
            "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
            "COMET ERROR: Failed to calculate active processors count. Fall back to default CPU count 1\n",
            "COMET INFO: Couldn't find a Git repository in '/content' nor in any parent directory. You can override where Comet is looking for a Git Patch by setting the configuration `COMET_GIT_DIRECTORY`\n",
            "COMET INFO: Experiment is live on comet.com https://www.comet.com/guiosorio/credit-card-fraud-curves/0c8d83ec29b64506bb5f3f88975c2a58\n",
            "\n",
            "COMET INFO: ---------------------------\n",
            "COMET INFO: Comet.ml Experiment Summary\n",
            "COMET INFO: ---------------------------\n",
            "COMET INFO:   Data:\n",
            "COMET INFO:     display_summary_level : 1\n",
            "COMET INFO:     url                   : https://www.comet.com/guiosorio/credit-card-fraud-curves/0c8d83ec29b64506bb5f3f88975c2a58\n",
            "COMET INFO:   Metrics [count] (min, max):\n",
            "COMET INFO:     accuracy [4]  : (0.9937676191329956, 0.9989641308784485)\n",
            "COMET INFO:     loss [4]      : (0.0026419416535645723, 0.003925579134374857)\n",
            "COMET INFO:     precision [4] : (0.1, 0.66)\n",
            "COMET INFO:     recall [4]    : (0.4266666666666667, 0.4666666666666667)\n",
            "COMET INFO:   Others:\n",
            "COMET INFO:     Name : 16 Repeats\n",
            "COMET INFO:   Parameters:\n",
            "COMET INFO:     Repeats : 16\n",
            "COMET INFO:   Uploads:\n",
            "COMET INFO:     confusion-matrix    : 1\n",
            "COMET INFO:     environment details : 1\n",
            "COMET INFO:     filename            : 1\n",
            "COMET INFO:     installed packages  : 1\n",
            "COMET INFO:     notebook            : 2\n",
            "COMET INFO:     os packages         : 1\n",
            "COMET INFO:     source_code         : 1\n",
            "COMET INFO: ---------------------------\n",
            "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: sklearn, torch. Metrics and hyperparameters can still be logged using Experiment.log_metrics() and Experiment.log_parameters()\n",
            "COMET INFO: Uploading 1 metrics, params and output messages\n",
            "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: sklearn, torch. Metrics and hyperparameters can still be logged using Experiment.log_metrics() and Experiment.log_parameters()\n",
            "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
            "COMET ERROR: Failed to calculate active processors count. Fall back to default CPU count 1\n",
            "COMET INFO: Couldn't find a Git repository in '/content' nor in any parent directory. You can override where Comet is looking for a Git Patch by setting the configuration `COMET_GIT_DIRECTORY`\n",
            "COMET INFO: Experiment is live on comet.com https://www.comet.com/guiosorio/credit-card-fraud-curves/5cfdb4d450ca42e4b6114615489ed8b2\n",
            "\n",
            "COMET INFO: ---------------------------\n",
            "COMET INFO: Comet.ml Experiment Summary\n",
            "COMET INFO: ---------------------------\n",
            "COMET INFO:   Data:\n",
            "COMET INFO:     display_summary_level : 1\n",
            "COMET INFO:     url                   : https://www.comet.com/guiosorio/credit-card-fraud-curves/5cfdb4d450ca42e4b6114615489ed8b2\n",
            "COMET INFO:   Metrics [count] (min, max):\n",
            "COMET INFO:     accuracy [4]  : (0.9910991191864014, 0.9989641308784485)\n",
            "COMET INFO:     loss [4]      : (0.00254778447560966, 0.006869828328490257)\n",
            "COMET INFO:     precision [4] : (0.08620689655172414, 0.8076923076923077)\n",
            "COMET INFO:     recall [4]    : (0.28, 0.6266666666666667)\n",
            "COMET INFO:   Others:\n",
            "COMET INFO:     Name : 21 Repeats\n",
            "COMET INFO:   Parameters:\n",
            "COMET INFO:     Repeats : 21\n",
            "COMET INFO:   Uploads:\n",
            "COMET INFO:     confusion-matrix    : 1\n",
            "COMET INFO:     environment details : 1\n",
            "COMET INFO:     filename            : 1\n",
            "COMET INFO:     installed packages  : 1\n",
            "COMET INFO:     notebook            : 2\n",
            "COMET INFO:     os packages         : 1\n",
            "COMET INFO:     source_code         : 1\n",
            "COMET INFO: ---------------------------\n",
            "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: sklearn, torch. Metrics and hyperparameters can still be logged using Experiment.log_metrics() and Experiment.log_parameters()\n",
            "COMET INFO: Uploading 1 metrics, params and output messages\n"
          ]
        }
      ],
      "source": [
        "# FIT\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "from comet_ml import Experiment\n",
        "\n",
        "for n in range(1, 22, 5):\n",
        "  # Create experiment\n",
        "  experiment = Experiment(\n",
        "    api_key=\"tSIIIrf40FyA0qDCJeLLC5jZP\",\n",
        "    project_name=\"credit-card-fraud-curves\",\n",
        "    workspace=\"guiosorio\",\n",
        "  )\n",
        "  experiment.set_name(f'{n} Repeats')\n",
        "\n",
        "  from flax.training import train_state\n",
        "  seed = 0\n",
        "  lr = 0.01 # lower learning rate with batch norm\n",
        "  n_epochs = 4\n",
        "\n",
        "  train_state = create_train_state(jax.random.PRNGKey(seed), lr)\n",
        "\n",
        "  train_loader_mod, test_features, test_lbls = mod_data(n)\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "    train_state, train_metrics = train_one_epoch(train_state, train_loader_mod)\n",
        "\n",
        "    test_metrics, test_preds, test_logits = evaluate_model(train_state, test_features, test_lbls)\n",
        "\n",
        "    # Precision\n",
        "    precision = precision_score(test_lbls, test_preds)\n",
        "    test_metrics['precision'] = precision\n",
        "    # Recall\n",
        "    recall = recall_score(test_lbls, test_preds)\n",
        "    test_metrics['recall'] = recall\n",
        "\n",
        "    # one_hot_test_lbls = jax.nn.one_hot(test_lbls, num_classes=2)\n",
        "    # log_curves(np.array(np.array(one_hot_test_lbls)), np.array(test_logits), epoch, n)\n",
        "\n",
        "    # Log metrics\n",
        "    experiment.log_metrics(test_metrics, step=epoch)\n",
        "\n",
        "  # Log confusion matrix\n",
        "  experiment.log_confusion_matrix(np.array(test_lbls).astype(int), np.array(test_preds).astype(int))\n",
        "  # Log parameter\n",
        "  experiment.log_parameter('Repeats', n)\n",
        "\n",
        "  experiment.end()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## COMET ANALYSIS (after 4 epochs)\n",
        "\n",
        "#### RECALL RANKING\n",
        "  1. 21 repeats (0.60)\n",
        "  2. 16 repeats (0.43)\n",
        "  3. 11 repeats (0.32)\n",
        "  4. 6 repeats (0.32)\n",
        "  5. 1 repeat (0.05)\n",
        "\n",
        "#### PRECISION RANKING\n",
        "  1. 11 repeat (0.83)\n",
        "  2. 1 repeat (0.67)\n",
        "  3. 16 repeats (0.65)\n",
        "  4. 6 repeats (0.27)\n",
        "  5. 21 repeats(0.09)"
      ],
      "metadata": {
        "id": "U3YBQoKzdxi4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KEY TAKEAWAYS:\n",
        "\n",
        "- **Focal loss VS Cross-entropy**\n",
        "  - as seen in https://github.com/guiOsorio/Learning_JAX/blob/master/CC_FocalLoss.ipynb, the use of **focal loss** instead of regular cross-entropy loss seems to **improve recall** but **lower precision** of the model. This makes sense as focal loss is used to give more weight to TPs in this case, as the original data is imbalanced (percentage of TPs in the original data is of ~0.183%).\n",
        "- **N repeats hyperparameter**\n",
        "  - consists of replicating the original TPs n times in the training set\n",
        "  - **an increase in the hyperparameter 'n repeats'** generally resulted in the **improved recall**. This effect is, again, to be expected since increasing the number of TPs in the training set will result in more positive classifications by the model."
      ],
      "metadata": {
        "id": "SHxjlgjFY8xX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- hide api_key\n",
        "- push to github"
      ],
      "metadata": {
        "id": "6-DCXczSgqIx"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXKWO5PoZlB9GJGebyXRdb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}