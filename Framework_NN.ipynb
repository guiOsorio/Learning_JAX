{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6hbhyVIyCYbU9rUchyL2K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guiOsorio/Learning_JAX/blob/master/Framework_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "Bg6tv5i8ucnK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap, grad, value_and_grad\n",
        "from jax import random\n",
        "import jax\n",
        "from jax.scipy.special import logsumexp\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train basic PyTorch model"
      ],
      "metadata": {
        "id": "2d8inhepA0Ip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTensorDataset(Dataset):\n",
        "  def __init__(self, dataset):\n",
        "    [data_X, data_y] = dataset\n",
        "    X_tensor, y_tensor = data_X, data_y\n",
        "    tensors = (X_tensor, y_tensor)\n",
        "    assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n",
        "    self.tensors = tensors\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    x = self.tensors[0][index]\n",
        "\n",
        "    y = self.tensors[1][index]\n",
        "\n",
        "    return x, y\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.tensors[0].size(0)"
      ],
      "metadata": {
        "id": "5FnanvBha5TI"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = torch.tensor([\n",
        "    [2.7810836,2.550537003],\n",
        "    [1.465489372,2.362125076],\n",
        "    [3.396561688,4.400293529],\n",
        "    [1.38807019,1.850220317],\n",
        "    [3.06407232,3.005305973],\n",
        "    [7.627531214,2.759262235],\n",
        "    [5.332441248,2.088626775],\n",
        "    [6.922596716,1.77106367],\n",
        "    [8.675418651,-0.242068655],\n",
        "    [7.673756466,3.508563011]\n",
        "])\n",
        "\n",
        "train_y = torch.tensor([0., 0., 0., 0., 0., 1., 1., 1., 1., 1.])\n",
        "\n",
        "train_x.shape,train_y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGct23YKnCOt",
        "outputId": "83455060-8bfa-442d-ca6f-db38599a9d1b"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([10, 2]), torch.Size([10]))"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model variables\n",
        "input_size = train_x.size()[1]\n",
        "hidden_size = 1 # number of neurons in hidden layer\n",
        "output_size = 2 # binary = 2 possible solutions\n",
        "\n",
        "model = nn.Sequential(nn.Linear(input_size, hidden_size)\n",
        "                      ,nn.Sigmoid()\n",
        "                      ,nn.Linear(hidden_size, output_size))\n",
        "                      # ,nn.Sigmoid()) #why does adding sigmoid mess up the output?\n",
        "\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaHWQi7LplpJ",
        "outputId": "200ff483-978c-4e0a-c7e2-d75f5de0fdc6"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=2, out_features=1, bias=True)\n",
              "  (1): Sigmoid()\n",
              "  (2): Linear(in_features=1, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = [train_x, train_y]\n",
        "train_dset = CustomTensorDataset(train)\n",
        "dataloader = DataLoader(train_dset, shuffle=True, num_workers=2)\n",
        "# Define variables for training\n",
        "loss = nn.MSELoss() # mean squared error\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "wzPE-Kpftn8-"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "start = time.time()\n",
        "\n",
        "# Train PyTorch model\n",
        "for epoch in range(100):\n",
        "  running_loss = 0\n",
        "  for data in dataloader:\n",
        "    x, y = data\n",
        "    optimizer.zero_grad() # reset gradients\n",
        "    loss_val = loss(model(x), y) # calculate loss\n",
        "    loss_val.backward() # calculate gradients\n",
        "    optimizer.step() # update weights\n",
        "    running_loss += loss_val.item()\n",
        "  losses.append(running_loss)\n",
        "end = time.time()\n",
        "\n",
        "print(end-start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8v8j-xG7vsSV",
        "outputId": "77f2c200-6227-4a1e-9e77-52c1700a3886"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.559067726135254\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "E1wkqBUe4UxW",
        "outputId": "a3fd292b-9837-4448-df85-0ed4e61f64d5"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f4f950d56d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 94
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test PyTorch model on credit card dataset, keep track of time of training and prediction"
      ],
      "metadata": {
        "id": "9OM2YmugArXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "url = 'https://raw.githubusercontent.com/nsethi31/Kaggle-Data-Credit-Card-Fraud-Detection/master/creditcard.csv'\n",
        "df = pd.read_csv(url)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "idulwBTJrCzN",
        "outputId": "13ba209b-0d1d-431c-d76b-ba5397b2d134"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
              "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
              "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
              "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
              "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
              "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
              "\n",
              "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
              "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
              "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
              "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
              "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
              "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
              "\n",
              "        V26       V27       V28  Amount  Class  \n",
              "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
              "1  0.125895 -0.008983  0.014724    2.69      0  \n",
              "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
              "3 -0.221929  0.062723  0.061458  123.50      0  \n",
              "4  0.502292  0.219422  0.215153   69.99      0  \n",
              "\n",
              "[5 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f9f4f509-b57f-457e-9a8c-2c79c31de58b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>...</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>149.62</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>...</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>378.66</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.966272</td>\n",
              "      <td>-0.185226</td>\n",
              "      <td>1.792993</td>\n",
              "      <td>-0.863291</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>1.247203</td>\n",
              "      <td>0.237609</td>\n",
              "      <td>0.377436</td>\n",
              "      <td>-1.387024</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.108300</td>\n",
              "      <td>0.005274</td>\n",
              "      <td>-0.190321</td>\n",
              "      <td>-1.175575</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>-0.221929</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>123.50</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.0</td>\n",
              "      <td>-1.158233</td>\n",
              "      <td>0.877737</td>\n",
              "      <td>1.548718</td>\n",
              "      <td>0.403034</td>\n",
              "      <td>-0.407193</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.592941</td>\n",
              "      <td>-0.270533</td>\n",
              "      <td>0.817739</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.009431</td>\n",
              "      <td>0.798278</td>\n",
              "      <td>-0.137458</td>\n",
              "      <td>0.141267</td>\n",
              "      <td>-0.206010</td>\n",
              "      <td>0.502292</td>\n",
              "      <td>0.219422</td>\n",
              "      <td>0.215153</td>\n",
              "      <td>69.99</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 31 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f9f4f509-b57f-457e-9a8c-2c79c31de58b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f9f4f509-b57f-457e-9a8c-2c79c31de58b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f9f4f509-b57f-457e-9a8c-2c79c31de58b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Divide into features and labels\n",
        "df_x2 = df.iloc[:, 1:4]\n",
        "df_y2 = df['Class'].to_frame()\n",
        "\n",
        "total_points = df_y2.shape[0]\n",
        "split = round(total_points*0.6)\n",
        "\n",
        "# Convert to tensors\n",
        "train_x2 = torch.tensor(df_x2.values, dtype=torch.float32)[:split]\n",
        "train_y2 = torch.tensor(df_y2.values, dtype=torch.float32)[:split]\n",
        "\n",
        "test_x2 = torch.tensor(df_x2.values, dtype=torch.float32)[split:]\n",
        "test_y2 = torch.tensor(df_y2.values, dtype=torch.float32)[split:]\n",
        "\n",
        "train_x2.size(), train_y2.size(), test_x2.size(), test_y2.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhsZdgNHB1y8",
        "outputId": "3177ea9e-4403-427e-d765-d0ffc35d72e6"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([170884, 3]),\n",
              " torch.Size([170884, 1]),\n",
              " torch.Size([113923, 3]),\n",
              " torch.Size([113923, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define same model as the previous time\n",
        "input_size2 = train_x2.size()[1]\n",
        "hidden_size2 = 10 # number of neurons in hidden layer\n",
        "output_size2 = 1\n",
        "\n",
        "model2 = nn.Sequential(nn.Linear(input_size2, hidden_size2)\n",
        "                      ,nn.Sigmoid()\n",
        "                      ,nn.Linear(hidden_size2, output_size2)\n",
        "                      ,nn.Sigmoid()) #why does adding sigmoid mess up the output?\n",
        "\n",
        "model2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZD4rr8FxFAux",
        "outputId": "938a92f3-1552-42ea-f216-6ac2cbcef8d3"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=3, out_features=10, bias=True)\n",
              "  (1): Sigmoid()\n",
              "  (2): Linear(in_features=10, out_features=1, bias=True)\n",
              "  (3): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train2 = [train_x2, train_y2]\n",
        "train_dset2 = CustomTensorDataset(train2)\n",
        "dataloader2 = DataLoader(train_dset2, batch_size=100, shuffle=True)\n",
        "\n",
        "# Define variables for training\n",
        "loss2 = nn.MSELoss() # mean squared error\n",
        "optimizer2 = optim.SGD(model2.parameters(), lr=0.2)"
      ],
      "metadata": {
        "id": "rGKnjXwrGCPL"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses2 = []\n",
        "start2 = time.time()\n",
        "\n",
        "# Train PyTorch model\n",
        "for epoch in range(10):\n",
        "  running_loss = 0\n",
        "  for data in dataloader2:\n",
        "    x, y = data\n",
        "    optimizer2.zero_grad() # reset gradients\n",
        "    loss_val = loss2(model2(x), y) # calculate loss\n",
        "    loss_val.backward() # calculate gradients\n",
        "    optimizer2.step() # update weights\n",
        "    running_loss += loss_val.item()\n",
        "  losses2.append(running_loss)\n",
        "end2 = time.time()\n",
        "\n",
        "print(end2-start2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F26UyIf9G5Bi",
        "outputId": "c400bda3-a7da-4799-e9d8-0e03bf210273"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19.351577520370483\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(losses2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "bGJz3xkSG7-9",
        "outputId": "b09300e2-ef76-465f-d6ee-20918d9e8241"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f4f95361410>]"
            ]
          },
          "metadata": {},
          "execution_count": 100
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on test set\n",
        "correct = 0\n",
        "total = test_y2.size()[0]\n",
        "for features, label in zip(test_x2, test_y2):\n",
        "    pred = 0 if model2(features).item() < 0.5 else 1\n",
        "    l = label.item()\n",
        "    if pred == l:\n",
        "      correct += 1\n",
        "accuracy = correct / total\n",
        "print(accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNW_Tuz4j1u9",
        "outputId": "dc015151-8315-4579-ff45-26f73332f34d"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9988413226477533\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PyTorch model times\n",
        "##### With no hardware accelerator:\n",
        "- Model 1 (100 epochs, num_workers=2) -> **8.6883 s**\n",
        "- Model 2 (10 epochs, batch_size=100, num_workers=10) -> \n",
        "\n",
        "##### With GPU hardware accelerator:\n",
        "- Model 1 (100 epochs, num_workers=2) -> **7.9927 s**\n",
        "- Model 2 (10 epochs, batch_size=100, num_workers=10)\n",
        "  - **4.8472 s**\n",
        "  - **accuracy of 99.35%**\n",
        "- Model 2 (10 epochs, num_workers = 10) \n",
        "  - **179.3489 s** \n",
        "  - **accuracy of 99.55%**\n",
        "- Model 2 (10 epochs)\n",
        "  - **37.5481 s**\n",
        "  - **accuracy of 99.6%**\n",
        "\n",
        "Results are expected using the first 10000 data points of the original data with 3 features, with an 80/20 train test split.\n",
        "\n",
        "## Final Model\n",
        "\n",
        "- Model 2 on entire dataset with 60/40 split & batch_size=100\n",
        "  - **17.4195 s**\n",
        "  - **accuracy of 99.88%**\n",
        "\n",
        "Further exploration questions/todos:\n",
        "-  why does sigmoid activation in output layer not work on model 1?\n",
        "- what does num_workers do? (note how training of model was faster without workers)\n",
        "- any particular advantages to using nn.Sequential vs nn.Module?\n",
        "- GPU vs TPU, when is it best to use each\n",
        "- do a classification matrix on final model for evaluation\n",
        "- create JAX neural net for Titanic dataset and submit, note accuracy in test set\n"
      ],
      "metadata": {
        "id": "tKCP-fgrIMiu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build same model with JAX on credit card dataset - keep track of time of training and prediction and compare to PyTorch's model"
      ],
      "metadata": {
        "id": "gDeMb2IQA-tw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize parameters of the model\n",
        "seed = 0\n",
        "\n",
        "def init_params(layers_size, parent_key):\n",
        "\n",
        "  params = []\n",
        "  # From a parent key, generate different keys for each layer\n",
        "  keys = jax.random.split(parent_key, num=len(layers_size)-1) # understand better what split does/why is it useful\n",
        "\n",
        "  # Set sizes of layers in the model (inputs to layers and outputs to layers)\n",
        "  in_layers = layers_size[:-1]\n",
        "  out_of_layers = layers_size[1:]\n",
        "\n",
        "  for in_layer, out_of_layer, key in zip(in_layers, out_of_layers, keys):\n",
        "    weights_key, bias_key = jax.random.split(key)\n",
        "\n",
        "    # Initialize params to be an array [weights, bias]\n",
        "    #where the weights are n rows (number of neurons, outputs to layer, inputs to next layer) x m columns (number of inputs to layer, outputs from previous layer/features)\n",
        "    #bias are n rows x 1 column (one bias per neuron)\n",
        "    params.append([\n",
        "        0.01*jax.random.normal(weights_key, shape=(out_of_layer, in_layer)) # n x m matrix\n",
        "        ,0.01*jax.random.normal(bias_key, shape=(out_of_layer,)) # vector with n values\n",
        "    ])\n",
        "\n",
        "  return params\n",
        "key3 = jax.random.PRNGKey(seed)\n",
        "params3 = init_params([3,10,2], key3)\n",
        "\n",
        "# Go through each layer of the initialized params and check if the shape is the expected one\n",
        "jax.tree_map(lambda x: x.shape, params3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUB1MviT959t",
        "outputId": "7c2fbde0-c123-4323-9c7d-26843439cb02"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[(10, 3), (10,)], [(2, 10), (2,)]]"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform tensors to np arrays in dataloaders, tensors not compatible with JAX\n",
        "def custom_collate_fn(batch):\n",
        "    transposed_data = list(zip(*batch))\n",
        "\n",
        "    labels = np.stack(transposed_data[1])\n",
        "    features = np.stack(transposed_data[0])\n",
        "\n",
        "    return features, labels"
      ],
      "metadata": {
        "id": "qEbg5MIHHsnb"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert pd.dataframes to tensors\n",
        "train_x3 = torch.tensor(df_x2.values, dtype=torch.float32)[:split]\n",
        "train_y3 = torch.squeeze(torch.tensor(df_y2.values, dtype=torch.float32)[:split])\n",
        "\n",
        "test_x3 = torch.tensor(df_x2.values, dtype=torch.float32)[split:]\n",
        "test_y3 = torch.squeeze(torch.tensor(df_y2.values, dtype=torch.float32)[split:])\n",
        "\n",
        "train_x3.size(), train_y3.size(), test_x3.size(), test_y3.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhpqEkjXFcX5",
        "outputId": "875e44e7-d89b-4cfe-970f-8ecf0c0a234a"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([170884, 3]),\n",
              " torch.Size([170884]),\n",
              " torch.Size([113923, 3]),\n",
              " torch.Size([113923]))"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 2\n",
        "# Create train and test dataloaders\n",
        "train3 = [train_x3, train_y3]\n",
        "train_dset3 = CustomTensorDataset(train3)\n",
        "train_dl3 = DataLoader(train_dset3, batch_size=batch_size, collate_fn=custom_collate_fn, shuffle=True)\n",
        "\n",
        "test3 = [test_x3, test_y3] # same data as in last model\n",
        "test_dset3 = CustomTensorDataset(test3)\n",
        "test_dl3 = DataLoader(test_dset3, batch_size=batch_size, collate_fn=custom_collate_fn, shuffle=False)\n",
        "\n",
        "train_features = jnp.array(train_dset3.tensors[0]).reshape(len(train_dset3), -1)\n",
        "train_lbls = jnp.array(train_dset3.tensors[1])\n",
        "\n",
        "test_features = jnp.array(test_dset3.tensors[0]).reshape(len(test_dset3), -1)\n",
        "test_lbls = jnp.array(test_dset3.tensors[1])\n",
        "\n",
        "# Print shapes of each batch to see if it matches up with the expected shapes based on our batch size\n",
        "for x, y in train_dl3:\n",
        "  print(x.shape)\n",
        "  print(y.shape)\n",
        "  break\n",
        "print('----')\n",
        "for x, y in test_dl3:\n",
        "  print(x.shape)\n",
        "  print(y.shape)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioFiq1U9NFWm",
        "outputId": "0461c8c4-18dd-4ba1-9aca-6dbb99921ba6"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 3)\n",
            "(2,)\n",
            "----\n",
            "(2, 3)\n",
            "(2,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to predict using the initialized neural net\n",
        "def predict(params, x):\n",
        "  hiddens = params[:-1] # take all hidden layers of the model (only one in our case)\n",
        "  \n",
        "  # Create a variable that will forward through the network (except the final layer) and store its output, based on an initial input\n",
        "  output_x = x\n",
        "  # Forward pass of x into the hidden layers\n",
        "  for w, b in hiddens:\n",
        "    output_x = jax.nn.sigmoid(jnp.dot(w, output_x) + b)\n",
        "  # at the end of this for loop, we have the inputs to the final layer of the network\n",
        "\n",
        "  # Forward pass output layer \n",
        "  #(done separately because often the final activation is different. if activation is the same, this operation can be implemented in the for loop above)\n",
        "  ws_last, b_last = params[-1]\n",
        "  final_output = jnp.dot(ws_last, output_x) + b_last\n",
        "\n",
        "  return final_output - logsumexp(final_output) # final_output\n",
        "\n",
        "batched_predict = vmap(predict, in_axes=(None, 0))\n",
        "\n",
        "print(batched_predict(params3, train_features).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5bIorygOg9S",
        "outputId": "756e0d77-2646-4e9d-8b10-d6aab76d7932"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(170884, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function\n",
        "def loss_fn(params, features, gt_labels):\n",
        "  \n",
        "    predictions = batched_predict(params, features)\n",
        "    return -jnp.mean(predictions * gt_labels)\n",
        "\n",
        "\n",
        "for fs, lbls in test_dl3:\n",
        "  print(loss_fn(params3, fs, lbls))\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1rCcjCUMf8H",
        "outputId": "fc793397-5a75-492e-9bf3-45c33ad2d5dc"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Update function\n",
        "@jit # jit causes the function inputs to change type\n",
        "def update(params, features, labels, lr=0.01):\n",
        "  loss, grads = value_and_grad(loss_fn)(params, features, labels)\n",
        "\n",
        "  return loss, jax.tree_map(lambda p, g: p - lr*g, params, grads)"
      ],
      "metadata": {
        "id": "gl20By9uPXqW"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy function\n",
        "def accuracy(params, ds_features, ds_labels):\n",
        "  # Return accuracy of model's (params3) predictions based on test dataloader\n",
        "  pred_classes = jnp.argmax(batched_predict(params, ds_features), axis=1)\n",
        "  return jnp.mean(ds_labels == pred_classes)\n",
        "\n",
        "accuracy(params3, train_features, train_lbls)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BwJm3t8NHUt",
        "outputId": "3c251430-ff5a-45dc-9cc8-71d0ca03414c"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray(0.99789333, dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jnp.sum(jnp.argmax(batched_predict(params3, train_features), axis=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Q09QbZTSFmq",
        "outputId": "1598090f-b99c-41ab-bff9-133d4c4e2363"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray(0, dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "n_epochs = 50\n",
        "\n",
        "new_params = init_params([3,10,2], key3)\n",
        "\n",
        "print(f'Pre-training: test acc = {accuracy(new_params, test_features, test_lbls)}')\n",
        "start3 = time.time()\n",
        "for epoch in range(n_epochs):\n",
        "  running_loss = 0\n",
        "  for i, (features, labels) in enumerate(train_dl3):\n",
        "\n",
        "    gt_labels = jax.nn.one_hot(labels, 2)\n",
        "    loss, new_params = update(new_params, features, gt_labels)\n",
        "    running_loss += loss\n",
        "    \n",
        "  print(f'Epoch {epoch+1}, test acc = {accuracy(new_params, test_features, test_lbls)}, running loss = {running_loss}')\n",
        "\n",
        "end3 = time.time()\n",
        "\n",
        "print(end3 - start3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgNZChUuj3O8",
        "outputId": "fa84795f-667e-4968-8d06-f842359488d5"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-training: test acc = 0.9988413453102112\n",
            "Epoch 1, test acc = 0.9988413453102112, running loss = 498.7337951660156\n",
            "Epoch 2, test acc = 0.9988413453102112, running loss = 384.96795654296875\n",
            "Epoch 3, test acc = 0.9988413453102112, running loss = 376.65966796875\n",
            "Epoch 4, test acc = 0.9988413453102112, running loss = 367.9631652832031\n",
            "Epoch 5, test acc = 0.9988413453102112, running loss = 360.4655456542969\n",
            "Epoch 6, test acc = 0.9988413453102112, running loss = 353.675537109375\n",
            "Epoch 7, test acc = 0.9988413453102112, running loss = 348.84661865234375\n",
            "Epoch 8, test acc = 0.9988413453102112, running loss = 344.2140808105469\n",
            "Epoch 9, test acc = 0.9988325834274292, running loss = 342.33343505859375\n",
            "Epoch 10, test acc = 0.9988238215446472, running loss = 339.88287353515625\n",
            "Epoch 11, test acc = 0.9988150000572205, running loss = 337.918701171875\n",
            "Epoch 12, test acc = 0.9988150000572205, running loss = 336.05230712890625\n",
            "Epoch 13, test acc = 0.9987974762916565, running loss = 335.3132629394531\n",
            "Epoch 14, test acc = 0.9988150000572205, running loss = 333.49713134765625\n",
            "Epoch 15, test acc = 0.9988150000572205, running loss = 333.0069885253906\n",
            "Epoch 16, test acc = 0.9988238215446472, running loss = 332.0734558105469\n",
            "Epoch 17, test acc = 0.9988325834274292, running loss = 331.6549072265625\n",
            "Epoch 18, test acc = 0.9988501071929932, running loss = 331.0165100097656\n",
            "Epoch 19, test acc = 0.9988589286804199, running loss = 330.5497741699219\n",
            "Epoch 20, test acc = 0.9988589286804199, running loss = 330.0381774902344\n",
            "Epoch 21, test acc = 0.9989203214645386, running loss = 329.1832580566406\n",
            "Epoch 22, test acc = 0.9988589286804199, running loss = 329.2859802246094\n",
            "Epoch 23, test acc = 0.9988676905632019, running loss = 328.99212646484375\n",
            "Epoch 24, test acc = 0.9988676905632019, running loss = 328.94146728515625\n",
            "Epoch 25, test acc = 0.9988764524459839, running loss = 328.6673583984375\n",
            "Epoch 26, test acc = 0.9989379048347473, running loss = 328.6588134765625\n",
            "Epoch 27, test acc = 0.9988764524459839, running loss = 327.9818420410156\n",
            "Epoch 28, test acc = 0.9988764524459839, running loss = 328.0845031738281\n",
            "Epoch 29, test acc = 0.9989379048347473, running loss = 327.5455017089844\n",
            "Epoch 30, test acc = 0.9989203214645386, running loss = 327.15423583984375\n",
            "Epoch 31, test acc = 0.9989291429519653, running loss = 326.4771423339844\n",
            "Epoch 32, test acc = 0.9988764524459839, running loss = 326.8402099609375\n",
            "Epoch 33, test acc = 0.9989203214645386, running loss = 326.77874755859375\n",
            "Epoch 34, test acc = 0.9988764524459839, running loss = 326.61767578125\n",
            "Epoch 35, test acc = 0.9989291429519653, running loss = 326.39996337890625\n",
            "Epoch 36, test acc = 0.9989291429519653, running loss = 326.60040283203125\n",
            "Epoch 37, test acc = 0.9989379048347473, running loss = 326.7741394042969\n",
            "Epoch 38, test acc = 0.9989379048347473, running loss = 325.89288330078125\n",
            "Epoch 39, test acc = 0.9988852143287659, running loss = 325.954345703125\n",
            "Epoch 40, test acc = 0.9989291429519653, running loss = 325.4679870605469\n",
            "Epoch 41, test acc = 0.9989379048347473, running loss = 325.8975830078125\n",
            "Epoch 42, test acc = 0.9988940358161926, running loss = 325.5411376953125\n",
            "Epoch 43, test acc = 0.9989379048347473, running loss = 325.70745849609375\n",
            "Epoch 44, test acc = 0.9989379048347473, running loss = 325.6300964355469\n",
            "Epoch 45, test acc = 0.99897301197052, running loss = 324.74359130859375\n",
            "Epoch 46, test acc = 0.9989905953407288, running loss = 323.9165954589844\n",
            "Epoch 47, test acc = 0.998981773853302, running loss = 324.3627014160156\n",
            "Epoch 48, test acc = 0.998955488204956, running loss = 324.8586120605469\n",
            "Epoch 49, test acc = 0.998981773853302, running loss = 324.3406066894531\n",
            "Epoch 50, test acc = 0.9989905953407288, running loss = 325.4115905761719\n",
            "510.3781898021698\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def confusion_matrix(params, ds_features, ds_labels):\n",
        "\n",
        "  conf_matrix = {'TPs': 0, 'TNs': 0, 'FPs': 0, 'FNs': 0}\n",
        "  # Make predictions\n",
        "  pred_classes = jnp.argmax(batched_predict(params, ds_features), axis=1)\n",
        "  # print(pred_classes.sum())\n",
        "  # Count occurence of each type in confusion matrix\n",
        "  for i in range(len(pred_classes)):\n",
        "    label = ds_labels[i]\n",
        "    pred = pred_classes[i]\n",
        "\n",
        "    if label == 1. and pred == 1: # truly predicted positive\n",
        "        conf_matrix['TPs'] += 1\n",
        "    elif label == 0. and pred == 1: # falsely predicted positive\n",
        "        conf_matrix['FPs'] += 1\n",
        "    elif label == 1. and pred == 0: # falsely predicted negative\n",
        "        conf_matrix['FNs'] += 1\n",
        "    elif label == 0. and pred == 0: # truly predicted negative\n",
        "        conf_matrix['TNs'] += 1\n",
        "\n",
        "    if i % 1000 == 0:\n",
        "      print(i)\n",
        "  \n",
        "  # Accuracy (what fraction does it get right) = (# TP + # TN) / Total\n",
        "  accuracy = (conf_matrix['TPs'] + conf_matrix['TNs']) / sum(conf_matrix.values())\n",
        "  print('ACCURACY: ', round(accuracy, 3))\n",
        "  print(' ')\n",
        "  \n",
        "  # Precision (when it says 1, how often is it right) = # TP / (# TP + # FP)\n",
        "  precision = conf_matrix['TPs'] / (conf_matrix['TPs'] + conf_matrix['FPs'])\n",
        "  print('PRECISION: ', round(precision, 3))\n",
        "  print(' ')\n",
        "  \n",
        "  # Recall (what fraction of 1s does it get right) = # TP / (# TP + # FN)\n",
        "  recall = conf_matrix['TPs'] / (conf_matrix['TPs'] + conf_matrix['FNs'])\n",
        "  print('RECALL: ', round(recall, 3))\n",
        "  print(' ')\n",
        "  \n",
        "  # False positive rate (what fraction of 0s are called 1s) = # FP / (# FP + # TN)\n",
        "  fprate = conf_matrix['FPs'] / (conf_matrix['FPs'] + conf_matrix['TNs'])\n",
        "  print('FALSE POSITIVE RATE: ', round(fprate, 3))\n",
        "  print(' ')\n",
        "  \n",
        "  # False negative rate (what fraction of 1s are called 0s) = # FN / (# TP + # FN)\n",
        "  fnrate = conf_matrix['FNs'] / (conf_matrix['TPs'] + conf_matrix['FNs'])\n",
        "  print('FALSE NEGATIVE RATE: ', round(fnrate, 3))\n",
        "\n",
        "  # Plot confusion matrix\n",
        "  matrix_arr = [[conf_matrix['TNs'], conf_matrix['FPs']], [conf_matrix['FNs'], conf_matrix['TPs']]]\n",
        "  plt.imshow(matrix_arr, cmap = 'coolwarm', alpha = 0.5)\n",
        "  plt.xticks(np.arange(0, 2), ['0', '1'])\n",
        "  plt.yticks(np.arange(0, 2), ['0', '1'])\n",
        "\n",
        "  plt.text(-0.1, 0, matrix_arr[0][0], fontsize = 14) # TNs\n",
        "  plt.text(0.95, 0, matrix_arr[0][1], fontsize = 14) # FPs\n",
        "  plt.text(-0.1, 1, matrix_arr[1][0], fontsize = 14) # FNs\n",
        "  plt.text(0.95, 1, matrix_arr[1][1], fontsize = 14) # TPs\n",
        "\n",
        "  plt.xlabel('Predictions', fontsize=18)\n",
        "  plt.ylabel('Actuals', fontsize=18)\n",
        "  plt.title('Confusion Matrix', fontsize=18)\n",
        "  plt.show()\n",
        "\n",
        "  return conf_matrix\n",
        "\n",
        "confusion_matrix(new_params, test_features, test_lbls)\n",
        "\n",
        "# How do I improve the performance of the confusion matrix?\n",
        "# Does the GPU optimize the confusion matrix implementation above? (where I go through each item one by one)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Q24ao325aUGn",
        "outputId": "b7d90584-6268-4e6a-d9fa-8d4f538f40d0"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n",
            "18000\n",
            "19000\n",
            "20000\n",
            "21000\n",
            "22000\n",
            "23000\n",
            "24000\n",
            "25000\n",
            "26000\n",
            "27000\n",
            "28000\n",
            "29000\n",
            "30000\n",
            "31000\n",
            "32000\n",
            "33000\n",
            "34000\n",
            "35000\n",
            "36000\n",
            "37000\n",
            "38000\n",
            "39000\n",
            "40000\n",
            "41000\n",
            "42000\n",
            "43000\n",
            "44000\n",
            "45000\n",
            "46000\n",
            "47000\n",
            "48000\n",
            "49000\n",
            "50000\n",
            "51000\n",
            "52000\n",
            "53000\n",
            "54000\n",
            "55000\n",
            "56000\n",
            "57000\n",
            "58000\n",
            "59000\n",
            "60000\n",
            "61000\n",
            "62000\n",
            "63000\n",
            "64000\n",
            "65000\n",
            "66000\n",
            "67000\n",
            "68000\n",
            "69000\n",
            "70000\n",
            "71000\n",
            "72000\n",
            "73000\n",
            "74000\n",
            "75000\n",
            "76000\n",
            "77000\n",
            "78000\n",
            "79000\n",
            "80000\n",
            "81000\n",
            "82000\n",
            "83000\n",
            "84000\n",
            "85000\n",
            "86000\n",
            "87000\n",
            "88000\n",
            "89000\n",
            "90000\n",
            "91000\n",
            "92000\n",
            "93000\n",
            "94000\n",
            "95000\n",
            "96000\n",
            "97000\n",
            "98000\n",
            "99000\n",
            "100000\n",
            "101000\n",
            "102000\n",
            "103000\n",
            "104000\n",
            "105000\n",
            "106000\n",
            "107000\n",
            "108000\n",
            "109000\n",
            "110000\n",
            "111000\n",
            "112000\n",
            "113000\n",
            "ACCURACY:  0.999\n",
            " \n",
            "PRECISION:  0.84\n",
            " \n",
            "RECALL:  0.159\n",
            " \n",
            "FALSE POSITIVE RATE:  0.0\n",
            " \n",
            "FALSE NEGATIVE RATE:  0.841\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'TPs': 21, 'TNs': 113787, 'FPs': 4, 'FNs': 111}"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W5dUyLfah_G6"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build JAX model for Titanic dataset"
      ],
      "metadata": {
        "id": "_4JcTyYFADQo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lPpzPuuUZBhU"
      },
      "execution_count": 112,
      "outputs": []
    }
  ]
}