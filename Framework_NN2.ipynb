{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "10A--SGAMbL0v0kdr2kSdov20WsVkc6rQ",
      "authorship_tag": "ABX9TyMgmUIXVHaZWHVfsGMv3FnC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guiOsorio/Learning_JAX/blob/master/Framework_NN2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Bg6tv5i8ucnK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap, grad, value_and_grad\n",
        "from jax import random\n",
        "import jax\n",
        "from jax.scipy.special import logsumexp\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train basic PyTorch model"
      ],
      "metadata": {
        "id": "2d8inhepA0Ip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTensorDataset(Dataset):\n",
        "  def __init__(self, dataset):\n",
        "    [data_X, data_y] = dataset\n",
        "    X_tensor, y_tensor = data_X, data_y\n",
        "    tensors = (X_tensor, y_tensor)\n",
        "    assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n",
        "    self.tensors = tensors\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    x = self.tensors[0][index]\n",
        "\n",
        "    y = self.tensors[1][index]\n",
        "\n",
        "    return x, y\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.tensors[0].size(0)"
      ],
      "metadata": {
        "id": "5FnanvBha5TI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = torch.tensor([\n",
        "    [2.7810836,2.550537003],\n",
        "    [1.465489372,2.362125076],\n",
        "    [3.396561688,4.400293529],\n",
        "    [1.38807019,1.850220317],\n",
        "    [3.06407232,3.005305973],\n",
        "    [7.627531214,2.759262235],\n",
        "    [5.332441248,2.088626775],\n",
        "    [6.922596716,1.77106367],\n",
        "    [8.675418651,-0.242068655],\n",
        "    [7.673756466,3.508563011]\n",
        "])\n",
        "\n",
        "train_y = torch.tensor([0., 0., 0., 0., 0., 1., 1., 1., 1., 1.])\n",
        "\n",
        "train_x.shape,train_y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGct23YKnCOt",
        "outputId": "fa8117e9-23cf-4394-fcb2-032e79dc71ca"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([10, 2]), torch.Size([10]))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model variables\n",
        "input_size = train_x.size()[1]\n",
        "hidden_size = 1 # number of neurons in hidden layer\n",
        "output_size = 2 # binary = 2 possible solutions\n",
        "\n",
        "model = nn.Sequential(nn.Linear(input_size, hidden_size)\n",
        "                      ,nn.Sigmoid()\n",
        "                      ,nn.Linear(hidden_size, output_size))\n",
        "                      # ,nn.Sigmoid()) #why does adding sigmoid mess up the output?\n",
        "\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaHWQi7LplpJ",
        "outputId": "4d270610-6644-4570-c302-4bdf0b468e3c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=2, out_features=1, bias=True)\n",
              "  (1): Sigmoid()\n",
              "  (2): Linear(in_features=1, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = [train_x, train_y]\n",
        "train_dset = CustomTensorDataset(train)\n",
        "dataloader = DataLoader(train_dset, shuffle=True, num_workers=2)\n",
        "# Define variables for training\n",
        "loss = nn.MSELoss() # mean squared error\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "wzPE-Kpftn8-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "start = time.time()\n",
        "\n",
        "# Train PyTorch model\n",
        "for epoch in range(100):\n",
        "  running_loss = 0\n",
        "  for data in dataloader:\n",
        "    x, y = data\n",
        "    optimizer.zero_grad() # reset gradients\n",
        "    loss_val = loss(model(x), y) # calculate loss\n",
        "    loss_val.backward() # calculate gradients\n",
        "    optimizer.step() # update weights\n",
        "    running_loss += loss_val.item()\n",
        "  losses.append(running_loss)\n",
        "end = time.time()\n",
        "\n",
        "print(end-start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8v8j-xG7vsSV",
        "outputId": "5aec1179-2ca6-482b-a1f2-d3ac24bd20a1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11.909357070922852\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "E1wkqBUe4UxW",
        "outputId": "03f1f381-3bac-4a17-d9d9-d609205a9021"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f3baf2b67d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test PyTorch model on credit card dataset, keep track of time of training and prediction"
      ],
      "metadata": {
        "id": "9OM2YmugArXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "url = 'https://raw.githubusercontent.com/nsethi31/Kaggle-Data-Credit-Card-Fraud-Detection/master/creditcard.csv'\n",
        "df = pd.read_csv(url)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "idulwBTJrCzN",
        "outputId": "69c036e1-6bf8-4020-c32f-d526416979d2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
              "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
              "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
              "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
              "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
              "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
              "\n",
              "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
              "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
              "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
              "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
              "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
              "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
              "\n",
              "        V26       V27       V28  Amount  Class  \n",
              "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
              "1  0.125895 -0.008983  0.014724    2.69      0  \n",
              "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
              "3 -0.221929  0.062723  0.061458  123.50      0  \n",
              "4  0.502292  0.219422  0.215153   69.99      0  \n",
              "\n",
              "[5 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-17afb2b4-37e0-4256-aa53-82d4cec3353b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>...</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>149.62</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>...</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>378.66</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.966272</td>\n",
              "      <td>-0.185226</td>\n",
              "      <td>1.792993</td>\n",
              "      <td>-0.863291</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>1.247203</td>\n",
              "      <td>0.237609</td>\n",
              "      <td>0.377436</td>\n",
              "      <td>-1.387024</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.108300</td>\n",
              "      <td>0.005274</td>\n",
              "      <td>-0.190321</td>\n",
              "      <td>-1.175575</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>-0.221929</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>123.50</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.0</td>\n",
              "      <td>-1.158233</td>\n",
              "      <td>0.877737</td>\n",
              "      <td>1.548718</td>\n",
              "      <td>0.403034</td>\n",
              "      <td>-0.407193</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.592941</td>\n",
              "      <td>-0.270533</td>\n",
              "      <td>0.817739</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.009431</td>\n",
              "      <td>0.798278</td>\n",
              "      <td>-0.137458</td>\n",
              "      <td>0.141267</td>\n",
              "      <td>-0.206010</td>\n",
              "      <td>0.502292</td>\n",
              "      <td>0.219422</td>\n",
              "      <td>0.215153</td>\n",
              "      <td>69.99</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 31 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-17afb2b4-37e0-4256-aa53-82d4cec3353b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-17afb2b4-37e0-4256-aa53-82d4cec3353b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-17afb2b4-37e0-4256-aa53-82d4cec3353b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Divide into features and labels\n",
        "df_x2 = df.iloc[:, 1:4]\n",
        "df_y2 = df['Class'].to_frame()\n",
        "\n",
        "total_points = df_y2.shape[0]\n",
        "split = round(total_points*0.6)\n",
        "\n",
        "# Convert to tensors\n",
        "train_x2 = torch.tensor(df_x2.values, dtype=torch.float32)[:split]\n",
        "train_y2 = torch.tensor(df_y2.values, dtype=torch.float32)[:split]\n",
        "\n",
        "test_x2 = torch.tensor(df_x2.values, dtype=torch.float32)[split:]\n",
        "test_y2 = torch.tensor(df_y2.values, dtype=torch.float32)[split:]\n",
        "\n",
        "train_x2.size(), train_y2.size(), test_x2.size(), test_y2.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhsZdgNHB1y8",
        "outputId": "8c8bdbc5-724b-49a6-ed2e-f43a92b8697a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([170884, 3]),\n",
              " torch.Size([170884, 1]),\n",
              " torch.Size([113923, 3]),\n",
              " torch.Size([113923, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define same model as the previous time\n",
        "input_size2 = train_x2.size()[1]\n",
        "hidden_size2 = 10 # number of neurons in hidden layer\n",
        "output_size2 = 1\n",
        "\n",
        "model2 = nn.Sequential(nn.Linear(input_size2, hidden_size2)\n",
        "                      ,nn.Sigmoid()\n",
        "                      ,nn.Linear(hidden_size2, output_size2)\n",
        "                      ,nn.Sigmoid()) #why does adding sigmoid mess up the output?\n",
        "\n",
        "model2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZD4rr8FxFAux",
        "outputId": "075beb14-da92-439c-f479-0ef75ae7f0ae"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=3, out_features=10, bias=True)\n",
              "  (1): Sigmoid()\n",
              "  (2): Linear(in_features=10, out_features=1, bias=True)\n",
              "  (3): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train2 = [train_x2, train_y2]\n",
        "train_dset2 = CustomTensorDataset(train2)\n",
        "dataloader2 = DataLoader(train_dset2, batch_size=100, shuffle=True)\n",
        "\n",
        "# Define variables for training\n",
        "loss2 = nn.MSELoss() # mean squared error\n",
        "optimizer2 = optim.SGD(model2.parameters(), lr=0.2)"
      ],
      "metadata": {
        "id": "rGKnjXwrGCPL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses2 = []\n",
        "start2 = time.time()\n",
        "\n",
        "# Train PyTorch model\n",
        "for epoch in range(10):\n",
        "  running_loss = 0\n",
        "  for data in dataloader2:\n",
        "    x, y = data\n",
        "    optimizer2.zero_grad() # reset gradients\n",
        "    loss_val = loss2(model2(x), y) # calculate loss\n",
        "    loss_val.backward() # calculate gradients\n",
        "    optimizer2.step() # update weights\n",
        "    running_loss += loss_val.item()\n",
        "  losses2.append(running_loss)\n",
        "end2 = time.time()\n",
        "\n",
        "print(end2-start2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F26UyIf9G5Bi",
        "outputId": "d9b002d8-d716-4d5b-a812-7277c186a9c7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18.10177755355835\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(losses2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "bGJz3xkSG7-9",
        "outputId": "0f849527-f13b-4f13-86c3-b0628b8f8cbc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f3bae43d110>]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on test set\n",
        "correct = 0\n",
        "total = test_y2.size()[0]\n",
        "for features, label in zip(test_x2, test_y2):\n",
        "    pred = 0 if model2(features).item() < 0.5 else 1\n",
        "    l = label.item()\n",
        "    if pred == l:\n",
        "      correct += 1\n",
        "accuracy = correct / total\n",
        "print(accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNW_Tuz4j1u9",
        "outputId": "155cbb89-e450-493d-cf90-1398485395eb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9988413226477533\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PyTorch model times\n",
        "##### With no hardware accelerator:\n",
        "- Model 1 (100 epochs, num_workers=2) -> **8.6883 s**\n",
        "- Model 2 (10 epochs, batch_size=100, num_workers=10) -> \n",
        "\n",
        "##### With GPU hardware accelerator:\n",
        "- Model 1 (100 epochs, num_workers=2) -> **7.9927 s**\n",
        "- Model 2 (10 epochs, batch_size=100, num_workers=10)\n",
        "  - **4.8472 s**\n",
        "  - **accuracy of 99.35%**\n",
        "- Model 2 (10 epochs, num_workers = 10) \n",
        "  - **179.3489 s** \n",
        "  - **accuracy of 99.55%**\n",
        "- Model 2 (10 epochs)\n",
        "  - **37.5481 s**\n",
        "  - **accuracy of 99.6%**\n",
        "\n",
        "Results are expected using the first 10000 data points of the original data with 3 features, with an 80/20 train test split.\n",
        "\n",
        "## Final Model\n",
        "\n",
        "- Model 2 on entire dataset with 60/40 split & batch_size=100\n",
        "  - **17.4195 s**\n",
        "  - **accuracy of 99.88%**\n",
        "\n",
        "Further exploration questions/todos:\n",
        "-  why does sigmoid activation in output layer not work on model 1?\n",
        "- what does num_workers do? (note how training of model was faster without workers)\n",
        "- any particular advantages to using nn.Sequential vs nn.Module?\n",
        "- GPU vs TPU, when is it best to use each\n",
        "- do a classification matrix on final model for evaluation\n",
        "- create JAX neural net for Titanic dataset and submit, note accuracy in test set\n"
      ],
      "metadata": {
        "id": "tKCP-fgrIMiu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build same model with JAX on credit card dataset - keep track of time of training and prediction and compare to PyTorch's model"
      ],
      "metadata": {
        "id": "gDeMb2IQA-tw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize parameters of the model\n",
        "seed = 0\n",
        "\n",
        "def init_params(layers_size, parent_key):\n",
        "\n",
        "  params = []\n",
        "  # From a parent key, generate different keys for each layer\n",
        "  keys = jax.random.split(parent_key, num=len(layers_size)-1) # understand better what split does/why is it useful\n",
        "\n",
        "  # Set sizes of layers in the model (inputs to layers and outputs to layers)\n",
        "  in_layers = layers_size[:-1]\n",
        "  out_of_layers = layers_size[1:]\n",
        "\n",
        "  for in_layer, out_of_layer, key in zip(in_layers, out_of_layers, keys):\n",
        "    weights_key, bias_key = jax.random.split(key)\n",
        "\n",
        "    # Initialize params to be an array [weights, bias]\n",
        "    #where the weights are n rows (number of neurons, outputs to layer, inputs to next layer) x m columns (number of inputs to layer, outputs from previous layer/features)\n",
        "    #bias are n rows x 1 column (one bias per neuron)\n",
        "    params.append([\n",
        "        0.01*jax.random.normal(weights_key, shape=(out_of_layer, in_layer)) # n x m matrix\n",
        "        ,0.01*jax.random.normal(bias_key, shape=(out_of_layer,)) # vector with n values\n",
        "    ])\n",
        "\n",
        "  return params\n",
        "key3 = jax.random.PRNGKey(seed)\n",
        "params_t = init_params([3,100,2], key3)\n",
        "\n",
        "# Go through each layer of the initialized params and check if the shape is the expected one\n",
        "jax.tree_map(lambda x: x.shape, params_t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUB1MviT959t",
        "outputId": "240f8270-18ff-42e0-e547-3afe799e7532"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[(100, 3), (100,)], [(2, 100), (2,)]]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform tensors to np arrays in dataloaders, tensors not compatible with JAX\n",
        "def custom_collate_fn(batch):\n",
        "    transposed_data = list(zip(*batch))\n",
        "\n",
        "    labels = np.stack(transposed_data[1])\n",
        "    features = np.stack(transposed_data[0])\n",
        "\n",
        "    return features, labels"
      ],
      "metadata": {
        "id": "qEbg5MIHHsnb"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert pd.dataframes to tensors\n",
        "train_x3 = torch.tensor(df_x2.values, dtype=torch.float32)[:split]\n",
        "train_y3 = torch.squeeze(torch.tensor(df_y2.values, dtype=torch.float32)[:split])\n",
        "\n",
        "test_x3 = torch.tensor(df_x2.values, dtype=torch.float32)[split:]\n",
        "test_y3 = torch.squeeze(torch.tensor(df_y2.values, dtype=torch.float32)[split:])\n",
        "\n",
        "train_x3.size(), train_y3.size(), test_x3.size(), test_y3.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhpqEkjXFcX5",
        "outputId": "912d1ada-6fa7-4352-aed6-cbb256fb90f3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([170884, 3]),\n",
              " torch.Size([170884]),\n",
              " torch.Size([113923, 3]),\n",
              " torch.Size([113923]))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take first 10000 values for training and add all TPs to the data\n",
        "\n",
        "## Take index of all TPs\n",
        "TP_idxs = (train_y3 == 1.).nonzero(as_tuple=True)[0] # tensor of indexes of all TPs in the dataset\n",
        "\n",
        "## Eliminate indexes below 10000 (will already be in the training dsataset)\n",
        "TP_idxs = TP_idxs[TP_idxs > 10000]\n",
        "\n",
        "## Create training set of first 10000 data points\n",
        "train_xt = train_x3\n",
        "train_yt = train_y3\n",
        "\n",
        "## Add TPs with indexes in TP_idxs to training set\n",
        "train_xTPs = train_x3[TP_idxs]\n",
        "train_yTPs = train_y3[TP_idxs]\n",
        "\n",
        "## Concatenate training data with remaining TPs\n",
        "train_xtfinal = torch.cat((train_xt, train_xTPs.repeat(50,1)), 0)\n",
        "train_ytfinal = torch.cat((train_yt, train_yTPs.repeat(50)), 0)\n",
        "\n",
        "train_xtfinal.size(), train_ytfinal.size()\n",
        "# We just increased the number of TPs by around 50x, let's try training on this new data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMvc8MmsJdaS",
        "outputId": "a08c76dc-e95f-416b-ac48-71efc546c9aa"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([186984, 3]), torch.Size([186984]))"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size_t = 100\n",
        "# Create train and test dataloaders\n",
        "train_t = [train_xtfinal, train_ytfinal]\n",
        "train_dset_t = CustomTensorDataset(train_t)\n",
        "train_dl_t = DataLoader(train_dset_t, batch_size=batch_size_t, collate_fn=custom_collate_fn, shuffle=True)\n",
        "\n",
        "test_t = [test_x3, test_y3] # same data as in last model\n",
        "test_dset_t = CustomTensorDataset(test_t)\n",
        "test_dl_t = DataLoader(test_dset_t, batch_size=batch_size_t, collate_fn=custom_collate_fn, shuffle=False)\n",
        "\n",
        "train_features = jnp.array(train_dset_t.tensors[0]).reshape(len(train_dset_t), -1)\n",
        "train_lbls = jnp.array(train_dset_t.tensors[1])\n",
        "\n",
        "test_features = jnp.array(test_dset_t.tensors[0]).reshape(len(test_dset_t), -1)\n",
        "test_lbls = jnp.array(test_dset_t.tensors[1])\n",
        "\n",
        "# Print shapes of each batch to see if it matches up with the expected shapes based on our batch size\n",
        "for x, y in train_dl_t:\n",
        "  print(x.shape)\n",
        "  print(y.shape)\n",
        "  break\n",
        "print('----')\n",
        "for x, y in test_dl_t:\n",
        "  print(x.shape)\n",
        "  print(y.shape)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KoStjHoONp_",
        "outputId": "fb0024ef-c74b-4499-fff6-0d9c2900dc8a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 3)\n",
            "(100,)\n",
            "----\n",
            "(100, 3)\n",
            "(100,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to predict using the initialized neural net\n",
        "def predict(params, x):\n",
        "  hiddens = params[:-1] # take all hidden layers of the model (only one in our case)\n",
        "  \n",
        "  # Create a variable that will forward through the network (except the final layer) and store its output, based on an initial input\n",
        "  output_x = x\n",
        "  # Forward pass of x into the hidden layers\n",
        "  for w, b in hiddens:\n",
        "    output_x = jax.nn.sigmoid(jnp.dot(w, output_x) + b)\n",
        "  # at the end of this for loop, we have the inputs to the final layer of the network\n",
        "\n",
        "  # Forward pass output layer \n",
        "  #(done separately because often the final activation is different. if activation is the same, this operation can be implemented in the for loop above)\n",
        "  ws_last, b_last = params[-1]\n",
        "  final_output = jnp.dot(ws_last, output_x) + b_last\n",
        "\n",
        "  return final_output - logsumexp(final_output) # final_output\n",
        "\n",
        "batched_predict = vmap(predict, in_axes=(None, 0))\n",
        "\n",
        "print(batched_predict(params_t, train_features).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5bIorygOg9S",
        "outputId": "a3ef3cd9-3e2b-41b6-9713-60cb264d58df"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(186984, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function\n",
        "def loss_fn(params, features, gt_labels):\n",
        "  \n",
        "    predictions = batched_predict(params, features)\n",
        "    return -jnp.mean(predictions * gt_labels)\n",
        "\n",
        "\n",
        "for fs, lbls in test_dl_t:\n",
        "  print(fs.shape)\n",
        "  print(jax.nn.one_hot(lbls, 2).shape)\n",
        "  print('---')\n",
        "  print(loss_fn(params_t, fs, jax.nn.one_hot(lbls, 2)))\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1rCcjCUMf8H",
        "outputId": "066ff16f-a16b-4db9-adb7-ffdccbcef37f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 3)\n",
            "(100, 2)\n",
            "---\n",
            "0.32397273\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Update function\n",
        "@jit # jit causes the function inputs to change type\n",
        "def update(params, features, labels, lr=0.01):\n",
        "  loss, grads = value_and_grad(loss_fn)(params, features, labels)\n",
        "\n",
        "  return loss, jax.tree_map(lambda p, g: p - lr*g, params, grads)"
      ],
      "metadata": {
        "id": "gl20By9uPXqW"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy function\n",
        "def accuracy(params, ds_features, ds_labels):\n",
        "  # Return accuracy of model's (params3) predictions based on test dataloader\n",
        "  pred_classes = jnp.argmax(batched_predict(params, ds_features), axis=1)\n",
        "  return jnp.mean(ds_labels == pred_classes)\n",
        "\n",
        "accuracy(params_t, train_features, train_lbls)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BwJm3t8NHUt",
        "outputId": "637828c9-3271-4eaf-8bc1-94b21fd95a13"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray(0.9119711, dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jnp.sum(jnp.argmax(batched_predict(params_t, train_features), axis=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Q09QbZTSFmq",
        "outputId": "8ed43cf9-0627-4e84-a115-f43f4bd34130"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray(0, dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "n_epochs = 50\n",
        "\n",
        "new_params = init_params([3,100,100,2], key3)\n",
        "\n",
        "print(f'Pre-training: test acc = {accuracy(new_params, test_features, test_lbls)}')\n",
        "start3 = time.time()\n",
        "for epoch in range(n_epochs):\n",
        "  running_loss = 0\n",
        "  for i, (features, labels) in enumerate(train_dl_t):\n",
        "\n",
        "    gt_labels = jax.nn.one_hot(labels, 2)\n",
        "    loss, new_params = update(new_params, features, gt_labels)\n",
        "    running_loss += loss\n",
        "    \n",
        "  print(f'Epoch {epoch+1}, test acc = {accuracy(new_params, test_features, test_lbls)}, running loss = {running_loss}')\n",
        "\n",
        "end3 = time.time()\n",
        "\n",
        "print(end3 - start3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgNZChUuj3O8",
        "outputId": "7257afdf-59ae-45a9-fdcf-bfa5ed6ced2b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-training: test acc = 0.0011586773907765746\n",
            "Epoch 1, test acc = 0.9988413453102112, running loss = 280.9410095214844\n",
            "Epoch 2, test acc = 0.9988413453102112, running loss = 278.43084716796875\n",
            "Epoch 3, test acc = 0.9988413453102112, running loss = 278.23834228515625\n",
            "Epoch 4, test acc = 0.9988413453102112, running loss = 277.9466552734375\n",
            "Epoch 5, test acc = 0.9988413453102112, running loss = 277.4809875488281\n",
            "Epoch 6, test acc = 0.9988413453102112, running loss = 276.7340393066406\n",
            "Epoch 7, test acc = 0.9988413453102112, running loss = 275.5912780761719\n",
            "Epoch 8, test acc = 0.9988413453102112, running loss = 273.66412353515625\n",
            "Epoch 9, test acc = 0.9988413453102112, running loss = 270.2906188964844\n",
            "Epoch 10, test acc = 0.9988413453102112, running loss = 263.8170471191406\n",
            "Epoch 11, test acc = 0.9988413453102112, running loss = 250.0774383544922\n",
            "Epoch 12, test acc = 0.9988413453102112, running loss = 221.1905517578125\n",
            "Epoch 13, test acc = 0.9958568811416626, running loss = 178.460693359375\n",
            "Epoch 14, test acc = 0.9902917146682739, running loss = 146.99757385253906\n",
            "Epoch 15, test acc = 0.986561119556427, running loss = 133.7479248046875\n",
            "Epoch 16, test acc = 0.9839453101158142, running loss = 128.8418731689453\n",
            "Epoch 17, test acc = 0.9833835363388062, running loss = 126.78382873535156\n",
            "Epoch 18, test acc = 0.9817245006561279, running loss = 125.71488952636719\n",
            "Epoch 19, test acc = 0.9829621911048889, running loss = 125.03165435791016\n",
            "Epoch 20, test acc = 0.9816631078720093, running loss = 124.61168670654297\n",
            "Epoch 21, test acc = 0.9813207387924194, running loss = 124.27470397949219\n",
            "Epoch 22, test acc = 0.9823565483093262, running loss = 124.03736114501953\n",
            "Epoch 23, test acc = 0.9816016554832458, running loss = 123.88396453857422\n",
            "Epoch 24, test acc = 0.9820141792297363, running loss = 123.7365493774414\n",
            "Epoch 25, test acc = 0.981022298336029, running loss = 123.61495208740234\n",
            "Epoch 26, test acc = 0.9805834293365479, running loss = 123.52967071533203\n",
            "Epoch 27, test acc = 0.9819527268409729, running loss = 123.47330474853516\n",
            "Epoch 28, test acc = 0.9815753102302551, running loss = 123.3997573852539\n",
            "Epoch 29, test acc = 0.9822950959205627, running loss = 123.3420639038086\n",
            "Epoch 30, test acc = 0.9806711673736572, running loss = 123.29147338867188\n",
            "Epoch 31, test acc = 0.9805131554603577, running loss = 123.2308120727539\n",
            "Epoch 32, test acc = 0.9802586436271667, running loss = 123.17826843261719\n",
            "Epoch 33, test acc = 0.982040524482727, running loss = 123.11065673828125\n",
            "Epoch 34, test acc = 0.980451762676239, running loss = 123.04719543457031\n",
            "Epoch 35, test acc = 0.9812066555023193, running loss = 122.9759521484375\n",
            "Epoch 36, test acc = 0.9807853102684021, running loss = 122.91049194335938\n",
            "Epoch 37, test acc = 0.9807238578796387, running loss = 122.8361587524414\n",
            "Epoch 38, test acc = 0.9801971912384033, running loss = 122.75611877441406\n",
            "Epoch 39, test acc = 0.9810837507247925, running loss = 122.70211791992188\n",
            "Epoch 40, test acc = 0.9814172983169556, running loss = 122.58422088623047\n",
            "Epoch 41, test acc = 0.9812329411506653, running loss = 122.48902893066406\n",
            "Epoch 42, test acc = 0.9804868698120117, running loss = 122.38567352294922\n",
            "Epoch 43, test acc = 0.9805570840835571, running loss = 122.29069519042969\n",
            "Epoch 44, test acc = 0.9807062745094299, running loss = 122.17310333251953\n",
            "Epoch 45, test acc = 0.9812856316566467, running loss = 122.03968811035156\n",
            "Epoch 46, test acc = 0.9798548221588135, running loss = 121.920166015625\n",
            "Epoch 47, test acc = 0.9802586436271667, running loss = 121.77799987792969\n",
            "Epoch 48, test acc = 0.9803376197814941, running loss = 121.62490844726562\n",
            "Epoch 49, test acc = 0.981478750705719, running loss = 121.47127532958984\n",
            "Epoch 50, test acc = 0.9809345006942749, running loss = 121.314453125\n",
            "177.43950867652893\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def confusion_matrix(params, ds_features, ds_labels):\n",
        "\n",
        "  conf_matrix = {'TPs': 0, 'TNs': 0, 'FPs': 0, 'FNs': 0}\n",
        "  # Make predictions\n",
        "  pred_classes = jnp.argmax(batched_predict(params, ds_features), axis=1)\n",
        "  # print(pred_classes.sum())\n",
        "  # Count occurence of each type in confusion matrix\n",
        "  for i in range(len(pred_classes)):\n",
        "    label = ds_labels[i]\n",
        "    pred = pred_classes[i]\n",
        "\n",
        "    if label == 1. and pred == 1: # truly predicted positive\n",
        "        conf_matrix['TPs'] += 1\n",
        "    elif label == 0. and pred == 1: # falsely predicted positive\n",
        "        conf_matrix['FPs'] += 1\n",
        "    elif label == 1. and pred == 0: # falsely predicted negative\n",
        "        conf_matrix['FNs'] += 1\n",
        "    elif label == 0. and pred == 0: # truly predicted negative\n",
        "        conf_matrix['TNs'] += 1\n",
        "\n",
        "    if i % 10000 == 0:\n",
        "      print(i)\n",
        "\n",
        "  print(conf_matrix)\n",
        "  \n",
        "  # Accuracy (what fraction does it get right) = (# TP + # TN) / Total\n",
        "  accuracy = (conf_matrix['TPs'] + conf_matrix['TNs']) / sum(conf_matrix.values())\n",
        "  print('ACCURACY: ', round(accuracy, 3))\n",
        "  print(' ')\n",
        "  \n",
        "  # Precision (when it says 1, how often is it right) = # TP / (# TP + # FP)\n",
        "  precision = conf_matrix['TPs'] / (conf_matrix['TPs'] + conf_matrix['FPs'])\n",
        "  print('PRECISION: ', round(precision, 3))\n",
        "  print(' ')\n",
        "  \n",
        "  # Recall (what fraction of 1s does it get right) = # TP / (# TP + # FN)\n",
        "  recall = conf_matrix['TPs'] / (conf_matrix['TPs'] + conf_matrix['FNs'])\n",
        "  print('RECALL: ', round(recall, 3))\n",
        "  print(' ')\n",
        "  \n",
        "  # False positive rate (what fraction of 0s are called 1s) = # FP / (# FP + # TN)\n",
        "  fprate = conf_matrix['FPs'] / (conf_matrix['FPs'] + conf_matrix['TNs'])\n",
        "  print('FALSE POSITIVE RATE: ', round(fprate, 3))\n",
        "  print(' ')\n",
        "  \n",
        "  # False negative rate (what fraction of 1s are called 0s) = # FN / (# TP + # FN)\n",
        "  fnrate = conf_matrix['FNs'] / (conf_matrix['TPs'] + conf_matrix['FNs'])\n",
        "  print('FALSE NEGATIVE RATE: ', round(fnrate, 3))\n",
        "\n",
        "  # Plot confusion matrix\n",
        "  matrix_arr = [[conf_matrix['TNs'], conf_matrix['FPs']], [conf_matrix['FNs'], conf_matrix['TPs']]]\n",
        "  plt.imshow(matrix_arr, cmap = 'coolwarm', alpha = 0.5)\n",
        "  plt.xticks(np.arange(0, 2), ['0', '1'])\n",
        "  plt.yticks(np.arange(0, 2), ['0', '1'])\n",
        "\n",
        "  plt.text(-0.1, 0, matrix_arr[0][0], fontsize = 14) # TNs\n",
        "  plt.text(0.95, 0, matrix_arr[0][1], fontsize = 14) # FPs\n",
        "  plt.text(-0.1, 1, matrix_arr[1][0], fontsize = 14) # FNs\n",
        "  plt.text(0.95, 1, matrix_arr[1][1], fontsize = 14) # TPs\n",
        "\n",
        "  plt.xlabel('Predictions', fontsize=18)\n",
        "  plt.ylabel('Actuals', fontsize=18)\n",
        "  plt.title('Confusion Matrix', fontsize=18)\n",
        "  plt.show()\n",
        "\n",
        "  return conf_matrix\n",
        "\n",
        "confusion_matrix(new_params, test_features, test_lbls)\n",
        "\n",
        "# How do I improve the performance of the confusion matrix?\n",
        "# Does the GPU optimize the confusion matrix implementation above? (where I go through each item one by one)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "id": "Q24ao325aUGn",
        "outputId": "58244864-7325-4212-9c71-5fda6e802977"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "10000\n",
            "20000\n",
            "30000\n",
            "40000\n",
            "50000\n",
            "60000\n",
            "70000\n",
            "80000\n",
            "90000\n",
            "100000\n",
            "110000\n",
            "{'TPs': 92, 'TNs': 111659, 'FPs': 2132, 'FNs': 40}\n",
            "ACCURACY:  0.981\n",
            " \n",
            "PRECISION:  0.041\n",
            " \n",
            "RECALL:  0.697\n",
            " \n",
            "FALSE POSITIVE RATE:  0.019\n",
            " \n",
            "FALSE NEGATIVE RATE:  0.303\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'TPs': 92, 'TNs': 111659, 'FPs': 2132, 'FNs': 40}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lPpzPuuUZBhU"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Run 1 - 50 epochs\n",
        "- Run 2 - TPs repeated 80 times, 15 epochs\n",
        "- Run 3 - TPs repeated 50 times, 50 epochs"
      ],
      "metadata": {
        "id": "ioIsCrZVgP6d"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8wg45u9PiKJE"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODOS\n",
        "\n",
        "- try CC model with focal loss (loss function which suits the dataset better - with low number of TPs)\n",
        "- try CC model with more TPs (meaning we copy the positive data points so the ratio of TPs/FPs is higher\n",
        "- analyze results with each and with both"
      ],
      "metadata": {
        "id": "VAvaFphOzTcU"
      }
    }
  ]
}