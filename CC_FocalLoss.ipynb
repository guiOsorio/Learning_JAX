{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNynbTHFjbd7PlptjxwJh/O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guiOsorio/Learning_JAX/blob/master/CC_FocalLoss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Focal loss: https://www.youtube.com/watch?v=Y8_OVwK4ECk\n",
        "- Focal loss for PyTorch: https://github.com/AdeelH/pytorch-multi-class-focal-loss"
      ],
      "metadata": {
        "id": "Diw1aeurAJwt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YZMP5OhU5zja"
      },
      "outputs": [],
      "source": [
        "# Install Flax and JAX\n",
        "!pip install --upgrade -q \"jax[cuda11_cudnn805]\" -f https://storage.googleapis.com/jax-releases/jax_releases.html\n",
        "!pip install --upgrade -q git+https://github.com/google/flax.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "from jax import lax, random, jit, numpy as jnp\n",
        "\n",
        "import flax\n",
        "from flax.core import freeze, unfreeze\n",
        "from flax import linen as nn\n",
        "from flax.training import train_state\n",
        "\n",
        "import optax\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import functools\n",
        "from typing import Sequence, Callable, Any, Optional\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "nfitRb8j55XE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "url = 'https://raw.githubusercontent.com/nsethi31/Kaggle-Data-Credit-Card-Fraud-Detection/master/creditcard.csv'\n",
        "df = pd.read_csv(url)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "kRgD8Vy76hnG",
        "outputId": "212eac58-411f-42d3-8af7-fc8bb1a84674"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
              "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
              "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
              "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
              "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
              "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
              "\n",
              "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
              "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
              "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
              "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
              "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
              "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
              "\n",
              "        V26       V27       V28  Amount  Class  \n",
              "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
              "1  0.125895 -0.008983  0.014724    2.69      0  \n",
              "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
              "3 -0.221929  0.062723  0.061458  123.50      0  \n",
              "4  0.502292  0.219422  0.215153   69.99      0  \n",
              "\n",
              "[5 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fa11beb1-9056-4f6c-8b81-5170ab74ee06\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>...</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>149.62</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>...</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>378.66</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.966272</td>\n",
              "      <td>-0.185226</td>\n",
              "      <td>1.792993</td>\n",
              "      <td>-0.863291</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>1.247203</td>\n",
              "      <td>0.237609</td>\n",
              "      <td>0.377436</td>\n",
              "      <td>-1.387024</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.108300</td>\n",
              "      <td>0.005274</td>\n",
              "      <td>-0.190321</td>\n",
              "      <td>-1.175575</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>-0.221929</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>123.50</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.0</td>\n",
              "      <td>-1.158233</td>\n",
              "      <td>0.877737</td>\n",
              "      <td>1.548718</td>\n",
              "      <td>0.403034</td>\n",
              "      <td>-0.407193</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.592941</td>\n",
              "      <td>-0.270533</td>\n",
              "      <td>0.817739</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.009431</td>\n",
              "      <td>0.798278</td>\n",
              "      <td>-0.137458</td>\n",
              "      <td>0.141267</td>\n",
              "      <td>-0.206010</td>\n",
              "      <td>0.502292</td>\n",
              "      <td>0.219422</td>\n",
              "      <td>0.215153</td>\n",
              "      <td>69.99</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 31 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fa11beb1-9056-4f6c-8b81-5170ab74ee06')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fa11beb1-9056-4f6c-8b81-5170ab74ee06 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fa11beb1-9056-4f6c-8b81-5170ab74ee06');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTensorDataset(Dataset):\n",
        "  def __init__(self, dataset):\n",
        "    [data_X, data_y] = dataset\n",
        "    X_tensor, y_tensor = data_X, data_y\n",
        "    tensors = (X_tensor, y_tensor)\n",
        "    assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n",
        "    self.tensors = tensors\n",
        "    self.data = tensors[0]\n",
        "    self.targets = tensors[1]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    x = self.tensors[0][index]\n",
        "\n",
        "    y = self.tensors[1][index]\n",
        "\n",
        "    return x, y\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.tensors[0].size(0)\n",
        "\n",
        "# Divide into features and labels\n",
        "df_x = df.iloc[:, 1:4]\n",
        "df_y = df['Class'].to_frame()\n",
        "\n",
        "total_points = df_y.shape[0]\n",
        "split = round(total_points*0.8)\n",
        "\n",
        "# Convert pd.dataframes to tensors\n",
        "train_x = torch.tensor(df_x.values, dtype=torch.float32)[:split]\n",
        "train_y = torch.squeeze(torch.tensor(df_y.values, dtype=torch.float32)[:split])\n",
        "\n",
        "test_x = torch.tensor(df_x.values, dtype=torch.float32)[split:]\n",
        "test_y = torch.squeeze(torch.tensor(df_y.values, dtype=torch.float32)[split:])\n",
        "\n",
        "train_x.size(), train_y.size(), test_x.size(), test_y.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAaz5NwP6jAZ",
        "outputId": "ab77014b-7c4e-4b4f-dd28-e1d1b84650ab"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([227846, 3]),\n",
              " torch.Size([227846]),\n",
              " torch.Size([56961, 3]),\n",
              " torch.Size([56961]))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform tensors to np arrays in dataloaders, tensors not compatible with JAX\n",
        "def custom_collate_fn(batch):\n",
        "    transposed_data = list(zip(*batch))\n",
        "\n",
        "    labels = np.stack(transposed_data[1])\n",
        "    features = np.stack(transposed_data[0])\n",
        "\n",
        "    return features, labels\n",
        "\n",
        "input_size = (1, 3)\n",
        "batch_size = 128\n",
        "\n",
        "train = [train_x, train_y]\n",
        "train_dset = CustomTensorDataset(train)\n",
        "train_loader = DataLoader(train_dset, collate_fn=custom_collate_fn, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test = [test_x, test_y]\n",
        "test_dset = CustomTensorDataset(test)\n",
        "test_loader = DataLoader(test_dset, collate_fn=custom_collate_fn, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# optimization - loading the whole dataset into memory\n",
        "train_features = jnp.array(train_dset.data)\n",
        "train_lbls = jnp.array(train_dset.targets)\n",
        "\n",
        "# np.expand_dims is to convert shape from (10000, 28, 28) -> (10000, 28, 28, 1)\n",
        "# We don't have to do this for training images because custom_transform does it for us.\n",
        "test_features = jnp.array(test_dset.data)\n",
        "test_lbls = jnp.array(test_dset.targets)\n",
        "\n",
        "## Create test loader\n",
        "\n",
        "for data in train_loader:\n",
        "  x, y = data\n",
        "  print(x.shape)\n",
        "  print(y.shape)\n",
        "  break\n",
        "\n",
        "print(test_features.shape)\n",
        "print(test_lbls.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ms7JWgI57gi",
        "outputId": "137f8acc-08d1-4ec2-ee7b-effd0e73fd59"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(128, 3)\n",
            "(128,)\n",
            "(56961, 3)\n",
            "(56961,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementation with batch norm and dropout\n",
        "class NN_regularized(nn.Module):\n",
        "\n",
        "  @nn.compact \n",
        "  def __call__(self, x, train: bool):\n",
        "    # Linear + dropout + relu\n",
        "    x = nn.Dense(features=100)(x)\n",
        "    x = nn.Dropout(0.2, deterministic=not train)(x)\n",
        "    x = nn.relu(x)\n",
        "\n",
        "    # Linear + batch norm + relu\n",
        "    x = nn.Dense(features=256)(x)\n",
        "    x = nn.BatchNorm(use_running_average=not train)(x)\n",
        "    x = nn.relu(x)\n",
        "\n",
        "    # Linear + softmax\n",
        "    x = nn.Dense(features=2)(x)\n",
        "    x = nn.log_softmax(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "ezBYGH8v6Kl-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING\n",
        "\n",
        "# Compute loss and update - this will be computed many times, so it's best to jit it\n",
        "@jit\n",
        "def training_state(state, imgs, gt_labels):\n",
        "\n",
        "  def crossEntropy_loss(params, batch_stats):\n",
        "    logits, updates = NN_regularized().apply({'params': params, 'batch_stats': batch_stats}, imgs, train=True, rngs={'dropout': jax.random.PRNGKey(0)}, mutable=['batch_stats'])\n",
        "    # logits is a vector of probabilities predicted by the model (the highest value in the vector is the prediction)\n",
        "    one_hot_gt_labels = jax.nn.one_hot(gt_labels, num_classes=2) # one hot encoded vector of labels\n",
        "    # logits.shape and one_hot_gt_labels shape is (batch_size, num_classes)\n",
        "    loss = -jnp.mean(jnp.sum(logits * one_hot_gt_labels, axis=-1)) # axis=-1 means sum over rows ||-> CE = true probability (one hot gt labels) * predicted probability (logits)\n",
        "\n",
        "    return loss, (logits, updates)\n",
        "  \n",
        "  (loss, (logits, updates)), grads = jax.value_and_grad(crossEntropy_loss, argnums=0, has_aux=True)(state.params, state.batch_stats)\n",
        "  state = state.apply_gradients(grads=grads) # update state params based on grads calculated\n",
        "  state = state.replace(batch_stats=updates['batch_stats']) # update state batch_stats variables\n",
        "\n",
        "  ## Accuracy\n",
        "  accuracy = jnp.mean(jnp.argmax(logits, -1) == gt_labels)\n",
        "  metrics = {\n",
        "      'loss': loss,\n",
        "      'accuracy': accuracy\n",
        "  }\n",
        "\n",
        "  return state, metrics\n",
        "\n",
        "# One epoch - need to add metrics part\n",
        "def train_one_epoch(state, dataloader):\n",
        "  batch_metrics = []\n",
        "  for cnt, (imgs, labels) in enumerate(dataloader):\n",
        "    state, metrics = training_state(state, imgs, labels)\n",
        "    batch_metrics.append(metrics)\n",
        "\n",
        "  batch_metrics_np = jax.device_get(batch_metrics)  # pull from the accelerator onto host (CPU)\n",
        "  epoch_metrics_np = {\n",
        "      k: np.mean([metrics[k] for metrics in batch_metrics_np])\n",
        "      for k in batch_metrics_np[0]\n",
        "  }\n",
        "\n",
        "  return state, epoch_metrics_np\n",
        "\n",
        "def create_train_state(key, lr, momentum):\n",
        "  # Create model\n",
        "  NN = NN_regularized()\n",
        "  # Initialize parameters\n",
        "  variables = NN.init(key, jnp.ones([1, *input_size]), train=False)\n",
        "  params = variables['params']\n",
        "  batch_stats_v = variables['batch_stats']\n",
        "  del variables\n",
        "\n",
        "  class TrainState_stats(train_state.TrainState):\n",
        "    batch_stats: Any\n",
        "\n",
        "  state = TrainState_stats.create(\n",
        "    apply_fn=NN.apply,\n",
        "    params=params,\n",
        "    batch_stats=batch_stats_v,\n",
        "    tx=optax.sgd(lr, momentum)\n",
        "  )\n",
        "\n",
        "  return state"
      ],
      "metadata": {
        "id": "hrs2zdHN9xoE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EVALUATION\n",
        "\n",
        "# Run one evaluation on test set\n",
        "@jit\n",
        "def eval_step(state, imgs, gt_labels):\n",
        "  logits = NN_regularized().apply({'params': state.params, 'batch_stats': state.batch_stats}, imgs, rngs={'dropout': jax.random.PRNGKey(0)}, train=False)\n",
        "  one_hot_gt_labels = jax.nn.one_hot(gt_labels, num_classes=2)\n",
        "  loss = -jnp.mean(jnp.sum(logits * one_hot_gt_labels, axis=-1))\n",
        "  preds = jnp.argmax(logits, -1)\n",
        "  accuracy = jnp.mean(preds == gt_labels)\n",
        "  metrics = {\n",
        "      'loss': loss,\n",
        "      'accuracy': accuracy\n",
        "  }\n",
        "  return metrics, preds\n",
        "\n",
        "def evaluate_model(state, test_imgs, test_labels):\n",
        "  metrics, preds = eval_step(state, test_imgs, test_labels)\n",
        "  metrics = jax.device_get(metrics) # pull from accelerator to CPU\n",
        "  metrics = jax.tree_map(lambda x: x.item(), metrics) # get scalar value from array\n",
        "  return metrics, preds"
      ],
      "metadata": {
        "id": "VYJCfodv9ysq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FIT 1 - no change in training data\n",
        "\n",
        "from flax.training import train_state\n",
        "seed = 0\n",
        "lr = 0.01 # lower learning rate with batch norm\n",
        "momentum = 0.9\n",
        "n_epochs = 4\n",
        "\n",
        "train_state = create_train_state(jax.random.PRNGKey(seed), lr, momentum)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  print(f'EPOCH {epoch+1}')\n",
        "\n",
        "  train_state, train_metrics = train_one_epoch(train_state, train_loader)\n",
        "  print(f'Train accuracy: {train_metrics[\"accuracy\"]}, Train loss: {train_metrics[\"loss\"]}')\n",
        "\n",
        "  test_metrics, test_preds = evaluate_model(train_state, test_features, test_lbls)\n",
        "  print(f'Test accuracy: {test_metrics[\"accuracy\"]}, Test loss: {test_metrics[\"loss\"]}')\n",
        "  print(' ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azA90aCz97v8",
        "outputId": "bde7ca27-297b-4613-eefa-00a8bb9c28e3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 1\n",
            "Train accuracy: 0.9969645142555237, Train loss: 0.013320038095116615\n",
            "Test accuracy: 0.9987534880638123, Test loss: 0.007097260560840368\n",
            " \n",
            "EPOCH 2\n",
            "Train accuracy: 0.9982410073280334, Train loss: 0.008100300095975399\n",
            "Test accuracy: 0.9989114999771118, Test loss: 0.0058660972863435745\n",
            " \n",
            "EPOCH 3\n",
            "Train accuracy: 0.9983243346214294, Train loss: 0.008032361045479774\n",
            "Test accuracy: 0.9991046190261841, Test loss: 0.007978584617376328\n",
            " \n",
            "EPOCH 4\n",
            "Train accuracy: 0.9983375072479248, Train loss: 0.007750450633466244\n",
            "Test accuracy: 0.9990519285202026, Test loss: 0.006333654280751944\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay, precision_score, recall_score\n",
        "\n",
        "ConfusionMatrixDisplay.from_predictions(test_lbls, test_preds)\n",
        "\n",
        "accuracy = test_metrics['accuracy']\n",
        "precision = precision_score(test_lbls, test_preds)\n",
        "recall = recall_score(test_lbls, test_preds)\n",
        "\n",
        "print(f'ACCURACY: {accuracy}')\n",
        "print(f'PRECISION: {precision}')\n",
        "print(f'RECALL: {recall}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "gqCbAZpIDoQG",
        "outputId": "5e14ef54-452b-41c5-c90c-a0f72ac7b02d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACCURACY: 0.9990519285202026\n",
            "PRECISION: 0.8181818181818182\n",
            "RECALL: 0.36\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build model with changes to fit data better\n",
        "\n",
        "- replicate TPs\n",
        "- implement focal loss instead of regular cross entropy"
      ],
      "metadata": {
        "id": "qBYbbP2pQOml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replicate each TP in the training set n times\n",
        "n = 10 # higher n => higher recall, lower precision\n",
        "\n",
        "TP_idxs = (train_y == 1).nonzero(as_tuple=True)[0]\n",
        "print(f'Percentage of TPs in original training data -> {(TP_idxs.shape[0] / train_y.shape[0]) * 100} %')\n",
        "\n",
        "extra_xs = train_x[TP_idxs].repeat(n, 1)\n",
        "extra_ys = train_y[TP_idxs].repeat(n,)\n",
        "\n",
        "train_x_mod = torch.cat((train_x, extra_xs), 0)\n",
        "train_y_mod = torch.cat((train_y, extra_ys), 0)\n",
        "\n",
        "print(f'Old shapes: {train_x.shape}, {train_y.shape}')\n",
        "print(f'New shapes: {train_x_mod.shape}, {train_y_mod.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VUD-V_QKNok",
        "outputId": "d4deebef-38a4-48cb-a6f2-6e4c5b271c62"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage of TPs in original training data -> 0.18301835450260262 %\n",
            "Old shapes: torch.Size([227846, 3]), torch.Size([227846])\n",
            "New shapes: torch.Size([232016, 3]), torch.Size([232016])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = (1, 3)\n",
        "batch_size = 128\n",
        "\n",
        "train_mod = [train_x_mod, train_y_mod]\n",
        "train_dset_mod = CustomTensorDataset(train_mod)\n",
        "train_loader_mod = DataLoader(train_dset_mod, collate_fn=custom_collate_fn, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test = [test_x, test_y]\n",
        "test_dset = CustomTensorDataset(test)\n",
        "test_loader = DataLoader(test_dset, collate_fn=custom_collate_fn, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# optimization - loading the whole dataset into memory\n",
        "train_features = jnp.array(train_dset_mod.data)\n",
        "train_lbls = jnp.array(train_dset_mod.targets)\n",
        "\n",
        "# np.expand_dims is to convert shape from (10000, 28, 28) -> (10000, 28, 28, 1)\n",
        "# We don't have to do this for training images because custom_transform does it for us.\n",
        "test_features = jnp.array(test_dset.data)\n",
        "test_lbls = jnp.array(test_dset.targets)\n",
        "\n",
        "## Create test loader\n",
        "\n",
        "for data in train_loader_mod:\n",
        "  x, y = data\n",
        "  print(x.shape)\n",
        "  print(y.shape)\n",
        "  break\n",
        "\n",
        "print(test_features.shape)\n",
        "print(test_lbls.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZK4RQnMpJ4Xy",
        "outputId": "5c37aa48-4883-4f76-b2f9-7fa7088909c7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(128, 3)\n",
            "(128,)\n",
            "(56961, 3)\n",
            "(56961,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING\n",
        "\n",
        "# Compute loss and update - this will be computed many times, so it's best to jit it\n",
        "@jit\n",
        "def training_state(state, imgs, gt_labels):\n",
        "\n",
        "  def crossEntropy_loss(params, batch_stats):\n",
        "    logits, updates = NN_regularized().apply({'params': params, 'batch_stats': batch_stats}, imgs, train=True, rngs={'dropout': jax.random.PRNGKey(0)}, mutable=['batch_stats'])\n",
        "    # logits is a vector of probabilities predicted by the model (the highest value in the vector is the prediction)\n",
        "    one_hot_gt_labels = jax.nn.one_hot(gt_labels, num_classes=2) # one hot encoded vector of labels\n",
        "    # logits.shape and one_hot_gt_labels shape is (batch_size, num_classes)\n",
        "    loss = -jnp.mean(jnp.sum(logits * one_hot_gt_labels, axis=-1)) # axis=-1 means sum over rows ||-> CE = true probability (one hot gt labels) * predicted probability (logits)\n",
        "\n",
        "    #### TODO \n",
        "    # Cross entropy to focal loss -> -log(pt) TO -log(pt) * (1-pt)^(gamma)\n",
        "    # above is -log(pt), need to find (1-pt)^gamma. log10(x) = 2 ==> x = 10^2, therefore log10(x) = logit ==> x = 10^logit\n",
        "    gamma = 2\n",
        "    probs_t = jnp.power(10, logits)\n",
        "    rev_probs_t = jnp.power(1-probs_t, gamma)\n",
        "    ####\n",
        "\n",
        "    return loss, (logits, updates)\n",
        "  \n",
        "  (loss, (logits, updates)), grads = jax.value_and_grad(crossEntropy_loss, argnums=0, has_aux=True)(state.params, state.batch_stats)\n",
        "  state = state.apply_gradients(grads=grads) # update state params based on grads calculated\n",
        "  state = state.replace(batch_stats=updates['batch_stats']) # update state batch_stats variables\n",
        "\n",
        "  ## Accuracy\n",
        "  accuracy = jnp.mean(jnp.argmax(logits, -1) == gt_labels)\n",
        "  metrics = {\n",
        "      'loss': loss,\n",
        "      'accuracy': accuracy\n",
        "  }\n",
        "\n",
        "  return state, metrics\n",
        "\n",
        "# One epoch - need to add metrics part\n",
        "def train_one_epoch(state, dataloader):\n",
        "  batch_metrics = []\n",
        "  for cnt, (imgs, labels) in enumerate(dataloader):\n",
        "    state, metrics = training_state(state, imgs, labels)\n",
        "    batch_metrics.append(metrics)\n",
        "\n",
        "  batch_metrics_np = jax.device_get(batch_metrics)  # pull from the accelerator onto host (CPU)\n",
        "  epoch_metrics_np = {\n",
        "      k: np.mean([metrics[k] for metrics in batch_metrics_np])\n",
        "      for k in batch_metrics_np[0]\n",
        "  }\n",
        "\n",
        "  return state, epoch_metrics_np\n",
        "\n",
        "def create_train_state(key, lr, momentum):\n",
        "  # Create model\n",
        "  NN = NN_regularized()\n",
        "  # Initialize parameters\n",
        "  variables = NN.init(key, jnp.ones([1, *input_size]), train=False)\n",
        "  params = variables['params']\n",
        "  batch_stats_v = variables['batch_stats']\n",
        "  del variables\n",
        "\n",
        "  class TrainState_stats(train_state.TrainState):\n",
        "    batch_stats: Any\n",
        "\n",
        "  state = TrainState_stats.create(\n",
        "    apply_fn=NN.apply,\n",
        "    params=params,\n",
        "    batch_stats=batch_stats_v,\n",
        "    tx=optax.sgd(lr, momentum)\n",
        "  )\n",
        "\n",
        "  return state"
      ],
      "metadata": {
        "id": "vRFiWVPyJ7qT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EVALUATION\n",
        "\n",
        "# Run one evaluation on test set\n",
        "@jit\n",
        "def eval_step(state, imgs, gt_labels):\n",
        "  logits = NN_regularized().apply({'params': state.params, 'batch_stats': state.batch_stats}, imgs, rngs={'dropout': jax.random.PRNGKey(0)}, train=False)\n",
        "  one_hot_gt_labels = jax.nn.one_hot(gt_labels, num_classes=2)\n",
        "  loss = -jnp.mean(jnp.sum(logits * one_hot_gt_labels, axis=-1))\n",
        "  preds = jnp.argmax(logits, -1)\n",
        "  accuracy = jnp.mean(preds == gt_labels)\n",
        "  metrics = {\n",
        "      'loss': loss,\n",
        "      'accuracy': accuracy\n",
        "  }\n",
        "  return metrics, preds\n",
        "\n",
        "def evaluate_model(state, test_imgs, test_labels):\n",
        "  metrics, preds = eval_step(state, test_imgs, test_labels)\n",
        "  metrics = jax.device_get(metrics) # pull from accelerator to CPU\n",
        "  metrics = jax.tree_map(lambda x: x.item(), metrics) # get scalar value from array\n",
        "  return metrics, preds"
      ],
      "metadata": {
        "id": "HtFS43HIQzjQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FIT 2 - replicate TPs + (TODO - focal loss)\n",
        "\n",
        "from flax.training import train_state\n",
        "seed = 0\n",
        "lr = 0.01 # lower learning rate with batch norm\n",
        "momentum = 0.9\n",
        "n_epochs = 4\n",
        "\n",
        "train_state = create_train_state(jax.random.PRNGKey(seed), lr, momentum)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  print(f'EPOCH {epoch+1}')\n",
        "\n",
        "  train_state, train_metrics = train_one_epoch(train_state, train_loader_mod)\n",
        "  print(f'Train accuracy: {train_metrics[\"accuracy\"]}, Train loss: {train_metrics[\"loss\"]}')\n",
        "\n",
        "  test_metrics, test_preds = evaluate_model(train_state, test_features, test_lbls)\n",
        "  print(f'Test accuracy: {test_metrics[\"accuracy\"]}, Test loss: {test_metrics[\"loss\"]}')\n",
        "  print(' ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpzyATHNQ3Tq",
        "outputId": "67eb41c2-15cf-4235-9318-f53261fe0f41"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 1\n",
            "Train accuracy: 0.9862236380577087, Train loss: 0.049358904361724854\n",
            "Test accuracy: 0.9957338571548462, Test loss: 0.030870113521814346\n",
            " \n",
            "EPOCH 2\n",
            "Train accuracy: 0.9881187677383423, Train loss: 0.043146006762981415\n",
            "Test accuracy: 0.9988763928413391, Test loss: 0.019584350287914276\n",
            " \n",
            "EPOCH 3\n",
            "Train accuracy: 0.9882566928863525, Train loss: 0.042162515223026276\n",
            "Test accuracy: 0.9973490238189697, Test loss: 0.02263578027486801\n",
            " \n",
            "EPOCH 4\n",
            "Train accuracy: 0.9887332916259766, Train loss: 0.04144949093461037\n",
            "Test accuracy: 0.9964887499809265, Test loss: 0.023896440863609314\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "ConfusionMatrixDisplay.from_predictions(test_lbls, test_preds)\n",
        "\n",
        "accuracy = test_metrics['accuracy']\n",
        "precision = precision_score(test_lbls, test_preds)\n",
        "recall = recall_score(test_lbls, test_preds)\n",
        "\n",
        "print(f'ACCURACY: {accuracy}')\n",
        "print(f'PRECISION: {precision}')\n",
        "print(f'RECALL: {recall}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "hUGvTmEYRJd0",
        "outputId": "3110353d-66a7-4fab-9633-7afac19f89ee"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACCURACY: 0.9964887499809265\n",
            "PRECISION: 0.19806763285024154\n",
            "RECALL: 0.5466666666666666\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}