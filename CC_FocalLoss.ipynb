{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOY3AUetXBSHqJUMjvVZKrp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guiOsorio/Learning_JAX/blob/master/CC_FocalLoss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Focal loss: https://www.youtube.com/watch?v=Y8_OVwK4ECk\n",
        "- Focal loss for PyTorch: https://github.com/AdeelH/pytorch-multi-class-focal-loss"
      ],
      "metadata": {
        "id": "Diw1aeurAJwt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZMP5OhU5zja",
        "outputId": "b7ce7fea-1e3d-43e3-9856-918bc83797d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 154 kB 4.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 65 kB 4.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 8.3 MB 50.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 237 kB 55.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 51 kB 6.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 85 kB 4.2 MB/s \n",
            "\u001b[?25h  Building wheel for flax (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Install Flax and JAX\n",
        "!pip install --upgrade -q \"jax[cuda11_cudnn805]\" -f https://storage.googleapis.com/jax-releases/jax_releases.html\n",
        "!pip install --upgrade -q git+https://github.com/google/flax.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "from jax import lax, random, jit, numpy as jnp\n",
        "\n",
        "import flax\n",
        "from flax.core import freeze, unfreeze\n",
        "from flax import linen as nn\n",
        "from flax.training import train_state\n",
        "\n",
        "import optax\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import functools\n",
        "from typing import Sequence, Callable, Any, Optional\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "nfitRb8j55XE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "url = 'https://raw.githubusercontent.com/nsethi31/Kaggle-Data-Credit-Card-Fraud-Detection/master/creditcard.csv'\n",
        "df = pd.read_csv(url)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "kRgD8Vy76hnG",
        "outputId": "a29203a7-eeda-46db-8326-64fd9afeb08c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
              "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
              "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
              "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
              "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
              "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
              "\n",
              "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
              "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
              "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
              "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
              "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
              "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
              "\n",
              "        V26       V27       V28  Amount  Class  \n",
              "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
              "1  0.125895 -0.008983  0.014724    2.69      0  \n",
              "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
              "3 -0.221929  0.062723  0.061458  123.50      0  \n",
              "4  0.502292  0.219422  0.215153   69.99      0  \n",
              "\n",
              "[5 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fadac8f8-cc0d-47ee-8188-9cb90f060c5e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>...</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>149.62</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>...</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>378.66</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.966272</td>\n",
              "      <td>-0.185226</td>\n",
              "      <td>1.792993</td>\n",
              "      <td>-0.863291</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>1.247203</td>\n",
              "      <td>0.237609</td>\n",
              "      <td>0.377436</td>\n",
              "      <td>-1.387024</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.108300</td>\n",
              "      <td>0.005274</td>\n",
              "      <td>-0.190321</td>\n",
              "      <td>-1.175575</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>-0.221929</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>123.50</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.0</td>\n",
              "      <td>-1.158233</td>\n",
              "      <td>0.877737</td>\n",
              "      <td>1.548718</td>\n",
              "      <td>0.403034</td>\n",
              "      <td>-0.407193</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.592941</td>\n",
              "      <td>-0.270533</td>\n",
              "      <td>0.817739</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.009431</td>\n",
              "      <td>0.798278</td>\n",
              "      <td>-0.137458</td>\n",
              "      <td>0.141267</td>\n",
              "      <td>-0.206010</td>\n",
              "      <td>0.502292</td>\n",
              "      <td>0.219422</td>\n",
              "      <td>0.215153</td>\n",
              "      <td>69.99</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 31 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fadac8f8-cc0d-47ee-8188-9cb90f060c5e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fadac8f8-cc0d-47ee-8188-9cb90f060c5e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fadac8f8-cc0d-47ee-8188-9cb90f060c5e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTensorDataset(Dataset):\n",
        "  def __init__(self, dataset):\n",
        "    [data_X, data_y] = dataset\n",
        "    X_tensor, y_tensor = data_X, data_y\n",
        "    tensors = (X_tensor, y_tensor)\n",
        "    assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n",
        "    self.tensors = tensors\n",
        "    self.data = tensors[0]\n",
        "    self.targets = tensors[1]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    x = self.tensors[0][index]\n",
        "\n",
        "    y = self.tensors[1][index]\n",
        "\n",
        "    return x, y\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.tensors[0].size(0)\n",
        "\n",
        "# Divide into features and labels\n",
        "df_x = df.iloc[:, 1:4]\n",
        "df_y = df['Class'].to_frame()\n",
        "\n",
        "total_points = df_y.shape[0]\n",
        "split = round(total_points*0.8)\n",
        "\n",
        "# Convert pd.dataframes to tensors\n",
        "train_x = torch.tensor(df_x.values, dtype=torch.float32)[:split]\n",
        "train_y = torch.squeeze(torch.tensor(df_y.values, dtype=torch.float32)[:split])\n",
        "\n",
        "test_x = torch.tensor(df_x.values, dtype=torch.float32)[split:]\n",
        "test_y = torch.squeeze(torch.tensor(df_y.values, dtype=torch.float32)[split:])\n",
        "\n",
        "train_x.size(), train_y.size(), test_x.size(), test_y.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAaz5NwP6jAZ",
        "outputId": "05223259-1e14-4f7b-a5c2-463e06f7f2fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([227846, 3]),\n",
              " torch.Size([227846]),\n",
              " torch.Size([56961, 3]),\n",
              " torch.Size([56961]))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform tensors to np arrays in dataloaders, tensors not compatible with JAX\n",
        "def custom_collate_fn(batch):\n",
        "    transposed_data = list(zip(*batch))\n",
        "\n",
        "    labels = np.stack(transposed_data[1])\n",
        "    features = np.stack(transposed_data[0])\n",
        "\n",
        "    return features, labels\n",
        "\n",
        "input_size = (1, 3)\n",
        "batch_size = 128\n",
        "\n",
        "train = [train_x, train_y]\n",
        "train_dset = CustomTensorDataset(train)\n",
        "train_loader = DataLoader(train_dset, collate_fn=custom_collate_fn, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test = [test_x, test_y]\n",
        "test_dset = CustomTensorDataset(test)\n",
        "test_loader = DataLoader(test_dset, collate_fn=custom_collate_fn, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# optimization - loading the whole dataset into memory\n",
        "train_features = jnp.array(train_dset.data)\n",
        "train_lbls = jnp.array(train_dset.targets)\n",
        "\n",
        "# np.expand_dims is to convert shape from (10000, 28, 28) -> (10000, 28, 28, 1)\n",
        "# We don't have to do this for training images because custom_transform does it for us.\n",
        "test_features = jnp.array(test_dset.data)\n",
        "test_lbls = jnp.array(test_dset.targets)\n",
        "\n",
        "## Create test loader\n",
        "\n",
        "for data in train_loader:\n",
        "  x, y = data\n",
        "  print(x.shape)\n",
        "  print(y.shape)\n",
        "  break\n",
        "\n",
        "print(test_features.shape)\n",
        "print(test_lbls.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ms7JWgI57gi",
        "outputId": "47543a78-a9fc-44a0-f8d3-da9773ecd6fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(128, 3)\n",
            "(128,)\n",
            "(56961, 3)\n",
            "(56961,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementation with batch norm and dropout\n",
        "class NN_regularized(nn.Module):\n",
        "\n",
        "  @nn.compact \n",
        "  def __call__(self, x, train: bool):\n",
        "    # Linear + dropout + relu\n",
        "    x = nn.Dense(features=100)(x)\n",
        "    x = nn.Dropout(0.2, deterministic=not train)(x)\n",
        "    x = nn.relu(x)\n",
        "\n",
        "    # Linear + batch norm + relu\n",
        "    x = nn.Dense(features=256)(x)\n",
        "    x = nn.BatchNorm(use_running_average=not train)(x)\n",
        "    x = nn.relu(x)\n",
        "\n",
        "    # Linear + softmax\n",
        "    x = nn.Dense(features=2)(x)\n",
        "    x = nn.softmax(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "ezBYGH8v6Kl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING\n",
        "\n",
        "# Compute loss and update - this will be computed many times, so it's best to jit it\n",
        "@jit\n",
        "def training_state(state, imgs, gt_labels):\n",
        "\n",
        "  def crossEntropy_loss(params, batch_stats):\n",
        "    probs, updates = NN_regularized().apply({'params': params, 'batch_stats': batch_stats}, imgs, train=True, rngs={'dropout': jax.random.PRNGKey(0)}, mutable=['batch_stats'])\n",
        "    logits = jnp.log10(probs)\n",
        "    # logits is a vector of probabilities predicted by the model (the highest value in the vector is the prediction)\n",
        "    one_hot_gt_labels = jax.nn.one_hot(gt_labels, num_classes=2) # one hot encoded vector of labels\n",
        "    # logits.shape and one_hot_gt_labels shape is (batch_size, num_classes)\n",
        "    loss = -jnp.mean(jnp.sum(logits * one_hot_gt_labels, axis=-1)) # axis=-1 means sum over rows ||-> CE = true probability (one hot gt labels) * predicted probability (logits)\n",
        "\n",
        "    # Add l2 regularization\n",
        "    alpha = 0.1\n",
        "    def l2_loss(weights, alpha):\n",
        "      return alpha * (weights ** 2).mean()\n",
        "    \n",
        "    loss += sum(\n",
        "        l2_loss(w, alpha)\n",
        "        for w in jax.tree_util.tree_leaves(params)\n",
        "      )\n",
        "\n",
        "    return loss, (logits, updates)\n",
        "  \n",
        "  (loss, (logits, updates)), grads = jax.value_and_grad(crossEntropy_loss, argnums=0, has_aux=True)(state.params, state.batch_stats)\n",
        "  state = state.apply_gradients(grads=grads) # update state params based on grads calculated\n",
        "  state = state.replace(batch_stats=updates['batch_stats']) # update state batch_stats variables\n",
        "\n",
        "  ## Accuracy\n",
        "  accuracy = jnp.mean(jnp.argmax(logits, -1) == gt_labels)\n",
        "  metrics = {\n",
        "      'loss': loss,\n",
        "      'accuracy': accuracy\n",
        "  }\n",
        "\n",
        "  return state, metrics\n",
        "\n",
        "# One epoch - need to add metrics part\n",
        "def train_one_epoch(state, dataloader):\n",
        "  batch_metrics = []\n",
        "  for cnt, (imgs, labels) in enumerate(dataloader):\n",
        "    state, metrics = training_state(state, imgs, labels)\n",
        "    batch_metrics.append(metrics)\n",
        "\n",
        "  batch_metrics_np = jax.device_get(batch_metrics)  # pull from the accelerator onto host (CPU)\n",
        "  epoch_metrics_np = {\n",
        "      k: np.mean([metrics[k] for metrics in batch_metrics_np])\n",
        "      for k in batch_metrics_np[0]\n",
        "  }\n",
        "\n",
        "  return state, epoch_metrics_np\n",
        "\n",
        "def create_train_state(key, lr, momentum):\n",
        "  # Create model\n",
        "  NN = NN_regularized()\n",
        "  # Initialize parameters\n",
        "  variables = NN.init(key, jnp.ones([1, *input_size]), train=False)\n",
        "  params = variables['params']\n",
        "  batch_stats_v = variables['batch_stats']\n",
        "  del variables\n",
        "\n",
        "  class TrainState_stats(train_state.TrainState):\n",
        "    batch_stats: Any\n",
        "\n",
        "  state = TrainState_stats.create(\n",
        "    apply_fn=NN.apply,\n",
        "    params=params,\n",
        "    batch_stats=batch_stats_v,\n",
        "    tx=optax.sgd(lr, momentum)\n",
        "  )\n",
        "\n",
        "  return state"
      ],
      "metadata": {
        "id": "hrs2zdHN9xoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EVALUATION\n",
        "\n",
        "# Run one evaluation on test set\n",
        "@jit\n",
        "def eval_step(state, imgs, gt_labels):\n",
        "  probs = NN_regularized().apply({'params': state.params, 'batch_stats': state.batch_stats}, imgs, rngs={'dropout': jax.random.PRNGKey(0)}, train=False)\n",
        "  logits = jnp.log10(probs)\n",
        "  one_hot_gt_labels = jax.nn.one_hot(gt_labels, num_classes=2)\n",
        "  loss = -jnp.mean(jnp.sum(logits * one_hot_gt_labels, axis=-1))\n",
        "  preds = jnp.argmax(logits, -1)\n",
        "  accuracy = jnp.mean(preds == gt_labels)\n",
        "  metrics = {\n",
        "      'loss': loss,\n",
        "      'accuracy': accuracy\n",
        "  }\n",
        "  return metrics, preds\n",
        "\n",
        "def evaluate_model(state, test_imgs, test_labels):\n",
        "  metrics, preds = eval_step(state, test_imgs, test_labels)\n",
        "  metrics = jax.device_get(metrics) # pull from accelerator to CPU\n",
        "  metrics = jax.tree_map(lambda x: x.item(), metrics) # get scalar value from array\n",
        "  return metrics, preds"
      ],
      "metadata": {
        "id": "VYJCfodv9ysq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FIT 1 - no change in training data\n",
        "\n",
        "from flax.training import train_state\n",
        "seed = 0\n",
        "lr = 0.01 # lower learning rate with batch norm\n",
        "momentum = 0.9\n",
        "n_epochs = 4\n",
        "\n",
        "train_state = create_train_state(jax.random.PRNGKey(seed), lr, momentum)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  print(f'EPOCH {epoch+1}')\n",
        "\n",
        "  train_state, train_metrics = train_one_epoch(train_state, train_loader)\n",
        "  print(f'Train accuracy: {train_metrics[\"accuracy\"]}, Train loss: {train_metrics[\"loss\"]}')\n",
        "\n",
        "  test_metrics, test_preds = evaluate_model(train_state, test_features, test_lbls)\n",
        "  print(f'Test accuracy: {test_metrics[\"accuracy\"]}, Test loss: {test_metrics[\"loss\"]}')\n",
        "  print(' ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azA90aCz97v8",
        "outputId": "42ef8051-e515-4f73-a252-ceee319f1990"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 1\n",
            "Train accuracy: 0.9964776039123535, Train loss: 0.1291336864233017\n",
            "Test accuracy: 0.998332142829895, Test loss: 0.005949253216385841\n",
            " \n",
            "EPOCH 2\n",
            "Train accuracy: 0.998162031173706, Train loss: 0.0980759710073471\n",
            "Test accuracy: 0.9982443451881409, Test loss: 0.00551892863586545\n",
            " \n",
            "EPOCH 3\n",
            "Train accuracy: 0.9982365965843201, Train loss: 0.07652987539768219\n",
            "Test accuracy: 0.9989466071128845, Test loss: 0.004323574248701334\n",
            " \n",
            "EPOCH 4\n",
            "Train accuracy: 0.9982014894485474, Train loss: 0.06018457189202309\n",
            "Test accuracy: 0.9989466071128845, Test loss: 0.004098046105355024\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay, precision_score, recall_score\n",
        "\n",
        "ConfusionMatrixDisplay.from_predictions(test_lbls, test_preds)\n",
        "\n",
        "accuracy = test_metrics['accuracy']\n",
        "precision = precision_score(test_lbls, test_preds)\n",
        "recall = recall_score(test_lbls, test_preds)\n",
        "\n",
        "print(f'ACCURACY: {accuracy}')\n",
        "print(f'PRECISION: {precision}')\n",
        "print(f'RECALL: {recall}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "gqCbAZpIDoQG",
        "outputId": "0c099f89-ce82-424a-e197-4cab39a342ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACCURACY: 0.9989466071128845\n",
            "PRECISION: 0.7419354838709677\n",
            "RECALL: 0.30666666666666664\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build model with changes to fit data better\n",
        "\n",
        "- replicate TPs -- DONE\n",
        "- implement focal loss instead of regular cross entropy"
      ],
      "metadata": {
        "id": "qBYbbP2pQOml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replicate each TP in the training set n times\n",
        "n = 1 # higher n => higher recall, lower precision\n",
        "\n",
        "TP_idxs = (train_y == 1).nonzero(as_tuple=True)[0]\n",
        "print(f'Percentage of TPs in original training data -> {(TP_idxs.shape[0] / train_y.shape[0]) * 100} %')\n",
        "\n",
        "extra_xs = train_x[TP_idxs].repeat(n, 1)\n",
        "extra_ys = train_y[TP_idxs].repeat(n,)\n",
        "\n",
        "train_x_mod = torch.cat((train_x, extra_xs), 0)\n",
        "train_y_mod = torch.cat((train_y, extra_ys), 0)\n",
        "\n",
        "print(f'Old shapes: {train_x.shape}, {train_y.shape}')\n",
        "print(f'New shapes: {train_x_mod.shape}, {train_y_mod.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VUD-V_QKNok",
        "outputId": "76386ac2-118f-4212-df7e-65a845859d32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage of TPs in original training data -> 0.18301835450260262 %\n",
            "Old shapes: torch.Size([227846, 3]), torch.Size([227846])\n",
            "New shapes: torch.Size([228263, 3]), torch.Size([228263])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = (1, 3)\n",
        "batch_size = 128\n",
        "\n",
        "train_mod = [train_x_mod, train_y_mod]\n",
        "train_dset_mod = CustomTensorDataset(train_mod)\n",
        "train_loader_mod = DataLoader(train_dset_mod, collate_fn=custom_collate_fn, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test = [test_x, test_y]\n",
        "test_dset = CustomTensorDataset(test)\n",
        "test_loader = DataLoader(test_dset, collate_fn=custom_collate_fn, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# optimization - loading the whole dataset into memory\n",
        "train_features = jnp.array(train_dset_mod.data)\n",
        "train_lbls = jnp.array(train_dset_mod.targets)\n",
        "\n",
        "# np.expand_dims is to convert shape from (10000, 28, 28) -> (10000, 28, 28, 1)\n",
        "# We don't have to do this for training images because custom_transform does it for us.\n",
        "test_features = jnp.array(test_dset.data)\n",
        "test_lbls = jnp.array(test_dset.targets)\n",
        "\n",
        "## Create test loader\n",
        "\n",
        "for data in train_loader_mod:\n",
        "  x, y = data\n",
        "  print(x.shape)\n",
        "  print(y.shape)\n",
        "  break\n",
        "\n",
        "print(test_features.shape)\n",
        "print(test_lbls.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZK4RQnMpJ4Xy",
        "outputId": "9de32d78-45bd-4b89-f567-d87bd0e66f96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(128, 3)\n",
            "(128,)\n",
            "(56961, 3)\n",
            "(56961,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING 2\n",
        "\n",
        "# Compute loss and update - this will be computed many times, so it's best to jit it\n",
        "@jit\n",
        "def training_state(state, imgs, gt_labels):\n",
        "\n",
        "  def FocalLoss(params, batch_stats):\n",
        "    probs, updates = NN_regularized().apply({'params': params, 'batch_stats': batch_stats}, imgs, train=True, rngs={'dropout': jax.random.PRNGKey(0)}, mutable=['batch_stats'])\n",
        "    logits = jnp.log10(probs)\n",
        "    # logits is a vector of probabilities predicted by the model (the highest value in the vector is the prediction)\n",
        "    one_hot_gt_labels = jax.nn.one_hot(gt_labels, num_classes=2) # one hot encoded vector of labels \n",
        "    # logits.shape and one_hot_gt_labels shape is (batch_size, num_classes)\n",
        "    # Cross entropy to focal loss -> -log(pt) TO -log(pt) * (1-pt)^(gamma)\n",
        "    # above is -log(pt), need to find (1-pt)^gamma. log10(x) = 2 ==> x = 10^2, therefore log10(x) = logit ==> x = 10^logit\n",
        "    gamma = 2\n",
        "    focal_loss = -jnp.mean(jnp.sum((logits * jnp.power(1-probs, gamma)) * one_hot_gt_labels, axis=-1)) \n",
        "    # axis=-1 means sum over rows ||-> CE = true probability (one hot gt labels) * predicted probability (logits)\n",
        "    \n",
        "\n",
        "    # Add l2 regularization\n",
        "    alpha = 0.1\n",
        "    def l2_loss(weights, alpha):\n",
        "      return alpha * (weights ** 2).mean()\n",
        "    \n",
        "    focal_loss += sum(\n",
        "        l2_loss(w, alpha)\n",
        "        for w in jax.tree_util.tree_leaves(params)\n",
        "      )\n",
        "\n",
        "    return focal_loss, (logits, updates)\n",
        "  \n",
        "  (loss, (logits, updates)), grads = jax.value_and_grad(FocalLoss, argnums=0, has_aux=True)(state.params, state.batch_stats)\n",
        "  state = state.apply_gradients(grads=grads) # update state params based on grads calculated\n",
        "  state = state.replace(batch_stats=updates['batch_stats']) # update state batch_stats variables\n",
        "\n",
        "  ## Accuracy\n",
        "  accuracy = jnp.mean(jnp.argmax(logits, -1) == gt_labels)\n",
        "  metrics = {\n",
        "      'loss': loss,\n",
        "      'accuracy': accuracy\n",
        "  }\n",
        "\n",
        "  return state, metrics\n",
        "\n",
        "# One epoch - need to add metrics part\n",
        "def train_one_epoch(state, dataloader):\n",
        "  batch_metrics = []\n",
        "  for cnt, (imgs, labels) in enumerate(dataloader):\n",
        "    state, metrics = training_state(state, imgs, labels)\n",
        "    batch_metrics.append(metrics)\n",
        "\n",
        "  batch_metrics_np = jax.device_get(batch_metrics)  # pull from the accelerator onto host (CPU)\n",
        "  epoch_metrics_np = {\n",
        "      k: np.mean([metrics[k] for metrics in batch_metrics_np])\n",
        "      for k in batch_metrics_np[0]\n",
        "  }\n",
        "\n",
        "  return state, epoch_metrics_np\n",
        "\n",
        "def create_train_state(key, lr, momentum):\n",
        "  # Create model\n",
        "  NN = NN_regularized()\n",
        "  # Initialize parameters\n",
        "  variables = NN.init(key, jnp.ones([1, *input_size]), train=False)\n",
        "  params = variables['params']\n",
        "  batch_stats_v = variables['batch_stats']\n",
        "  del variables\n",
        "\n",
        "  class TrainState_stats(train_state.TrainState):\n",
        "    batch_stats: Any\n",
        "\n",
        "  state = TrainState_stats.create(\n",
        "    apply_fn=NN.apply,\n",
        "    params=params,\n",
        "    batch_stats=batch_stats_v,\n",
        "    tx=optax.sgd(lr, momentum)\n",
        "  )\n",
        "\n",
        "  return state"
      ],
      "metadata": {
        "id": "vRFiWVPyJ7qT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EVALUATION 2\n",
        "\n",
        "# Run one evaluation on test set\n",
        "@jit\n",
        "def eval_step(state, imgs, gt_labels):\n",
        "  probs = NN_regularized().apply({'params': state.params, 'batch_stats': state.batch_stats}, imgs, rngs={'dropout': jax.random.PRNGKey(0)}, train=False)\n",
        "  logits = jnp.log10(probs)\n",
        "  one_hot_gt_labels = jax.nn.one_hot(gt_labels, num_classes=2)\n",
        "  gamma = 2\n",
        "  focal_loss = -jnp.mean(jnp.sum((logits * jnp.power(1-probs, gamma)) * one_hot_gt_labels, axis=-1)) \n",
        "  preds = jnp.argmax(logits, -1)\n",
        "  accuracy = jnp.mean(preds == gt_labels)\n",
        "  metrics = {\n",
        "      'loss': focal_loss,\n",
        "      'accuracy': accuracy\n",
        "  }\n",
        "  return metrics, preds\n",
        "\n",
        "def evaluate_model(state, test_imgs, test_labels):\n",
        "  metrics, preds = eval_step(state, test_imgs, test_labels)\n",
        "  metrics = jax.device_get(metrics) # pull from accelerator to CPU\n",
        "  metrics = jax.tree_map(lambda x: x.item(), metrics) # get scalar value from array\n",
        "  return metrics, preds"
      ],
      "metadata": {
        "id": "HtFS43HIQzjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FIT 2 - replicate TPs + (TODO - focal loss)\n",
        "\n",
        "from flax.training import train_state\n",
        "seed = 0\n",
        "lr = 0.01 # lower learning rate with batch norm\n",
        "momentum = 0.9\n",
        "n_epochs = 4\n",
        "\n",
        "train_state = create_train_state(jax.random.PRNGKey(seed), lr, momentum)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  print(f'EPOCH {epoch+1}')\n",
        "\n",
        "  train_state, train_metrics = train_one_epoch(train_state, train_loader_mod)\n",
        "  print(f'Train accuracy: {train_metrics[\"accuracy\"]}, Train loss: {train_metrics[\"loss\"]}')\n",
        "\n",
        "  test_metrics, test_preds = evaluate_model(train_state, test_features, test_lbls)\n",
        "  print(f'Test accuracy: {test_metrics[\"accuracy\"]}, Test loss: {test_metrics[\"loss\"]}')\n",
        "  print(' ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpzyATHNQ3Tq",
        "outputId": "c5309fa0-7a32-48c5-9855-e76147a84848"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 1\n",
            "Train accuracy: 0.9944208860397339, Train loss: 0.12494826316833496\n",
            "Test accuracy: 0.9974894523620605, Test loss: 0.0023358743637800217\n",
            " \n",
            "EPOCH 2\n",
            "Train accuracy: 0.9966486692428589, Train loss: 0.09527182579040527\n",
            "Test accuracy: 0.9974894523620605, Test loss: 0.002157089300453663\n",
            " \n",
            "EPOCH 3\n",
            "Train accuracy: 0.9967856407165527, Train loss: 0.07361941784620285\n",
            "Test accuracy: 0.9965063333511353, Test loss: 0.0025355916004627943\n",
            " \n",
            "EPOCH 4\n",
            "Train accuracy: 0.9968951344490051, Train loss: 0.057096660137176514\n",
            "Test accuracy: 0.9972963333129883, Test loss: 0.0021402298007160425\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "ConfusionMatrixDisplay.from_predictions(test_lbls, test_preds)\n",
        "\n",
        "accuracy = test_metrics['accuracy']\n",
        "precision = precision_score(test_lbls, test_preds)\n",
        "recall = recall_score(test_lbls, test_preds)\n",
        "\n",
        "print(f'ACCURACY: {accuracy}')\n",
        "print(f'PRECISION: {precision}')\n",
        "print(f'RECALL: {recall}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "hUGvTmEYRJd0",
        "outputId": "654dcdf0-bba1-454a-c231-101d993ef9cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACCURACY: 0.9972963333129883\n",
            "PRECISION: 0.24516129032258063\n",
            "RECALL: 0.5066666666666667\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}